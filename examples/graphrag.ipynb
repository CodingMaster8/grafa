{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 : Entity & RelationShips Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "from expertai.settings import OPENAI_API_KEY\n",
    "import os\n",
    "from datetime import datetime\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "\n",
    "# Langchain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "NEO4J_URI = 'neo4j+s://7fc9791f.databases.neo4j.io'\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "NEO4J_PASSWORD = 'YCCaF7BS6OGkTzKePeFIJ59l4Ai8HsaOntX4lAWz4XM'\n",
    "NEO4J_DATABASE = 'neo4j'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema of Knowledge Graph\n",
    "\n",
    "Node properties:\n",
    "\n",
    "Person {type: STRING, uuid: STRING, name: STRING, occupation: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "Company {type: STRING, uuid: STRING, name: STRING, country: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "\n",
    "Concept {type: STRING, uuid: STRING, name: STRING, synonyms: LIST, description: STRING, textEmbedding: LIST, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "Metric {type: STRING, uuid: STRING, name: STRING, description: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "Formula {type: STRING, uuid: STRING, name: STRING, formula: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "Rule {type: STRING, uuid: STRING, name: STRING, description: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "Condition {type: STRING, uuid: STRING, name: STRING, condition: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "Example {type: STRING, uuid: STRING, name: STRING, example: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "\n",
    "Chunk {type: STRING, uuid: STRING, name: STRING, data: STRING, textEmbedding: LIST, source: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
    "\n",
    "\n",
    "Relationship properties:\n",
    "\n",
    "The relationships are the following:\n",
    "(:Person)-[:WORKS_AT]->(:Company),\n",
    "(:Concept)-[:IS_RELATED_TO]->(:Concept),\n",
    "(:Concept)-[:IS_RELATED_TO]->(:Metric),\n",
    "(:Metric)-[:FORMULA]->(:Formula),\n",
    "(:Concept)-[:USES_RULE]->(:Rule),\n",
    "(:Rule)-[:IS_CONDITIONED]->(:Condition),\n",
    "(:Concept)-[:EXAMPLE]->(:Example),\n",
    "\n",
    "#### Reduced Schema for LLM\n",
    "Person {type: STRING, name: STRING, occupation: STRING}\n",
    "Company {type: STRING, name: STRING}\n",
    "\n",
    "Concept {type: STRING, name: STRING, synonyms: LIST, description: STRING}\n",
    "Metric {type: STRING, name: STRING, description: STRING}\n",
    "Formula {type: STRING, name: STRING, formula: STRING}\n",
    "Rule {type: STRING, name: STRING, description: STRING}\n",
    "Condition {type: STRING, name: STRING, condition: STRING}\n",
    "Example {type: STRING, name: STRING, example: STRING}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Neo4J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")\n",
    "\n",
    "llm_light = ChatBedrock(\n",
    "        model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "        model_kwargs=dict(temperature=0, max_tokens=4096),\n",
    "    )\n",
    "\n",
    "llm = ChatBedrock(\n",
    "        model_id=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        model_kwargs=dict(temperature=0, max_tokens=4096),\n",
    "    )\n",
    "\n",
    "#Probar los de amazon Nova si no haiku3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping constraint: unique_company_name\n",
      "Dropping constraint: unique_concept_name\n",
      "Dropping constraint: unique_condition_name\n",
      "Dropping constraint: unique_example_name\n",
      "Dropping constraint: unique_formula_name\n",
      "Dropping constraint: unique_metric_name\n",
      "Dropping constraint: unique_person_name\n",
      "Dropping constraint: unique_rule_name\n",
      "Dropping constraint: unqiue_chunk_name\n",
      "Dropping index: vector_chunks\n",
      "Dropping index: vector_concepts\n",
      "All schema properties (constraints and indexes) have been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Drop all constraints\n",
    "constraints = kg.query(\"SHOW CONSTRAINTS\")\n",
    "for constraint in constraints:\n",
    "    constraint_name = constraint[\"name\"]\n",
    "    print(f\"Dropping constraint: {constraint_name}\")\n",
    "    kg.query(f\"DROP CONSTRAINT {constraint_name}\")\n",
    "\n",
    "# Drop all indexes\n",
    "indexes = kg.query(\"SHOW INDEXES\")\n",
    "for index in indexes:\n",
    "    index_name = index[\"name\"]\n",
    "    print(f\"Dropping index: {index_name}\")\n",
    "    kg.query(f\"DROP INDEX {index_name}\")\n",
    "\n",
    "print(\"All schema properties (constraints and indexes) have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uniqueness constraint to avoid duplicate nodes\n",
    "def create_uniqueness_constraints(kg):\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_person_name IF NOT EXISTS \n",
    "        FOR (p:Person) REQUIRE p.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_company_name IF NOT EXISTS \n",
    "        FOR (c:Company) REQUIRE c.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_concept_name IF NOT EXISTS \n",
    "        FOR (co:Concept) REQUIRE co.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_metric_name IF NOT EXISTS \n",
    "        FOR (m:Metric) REQUIRE m.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_formula_name IF NOT EXISTS \n",
    "        FOR (f:Formula) REQUIRE f.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_rule_name IF NOT EXISTS \n",
    "        FOR (r:Rule) REQUIRE r.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_condition_name IF NOT EXISTS \n",
    "        FOR (cond:Condition) REQUIRE cond.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_example_name IF NOT EXISTS \n",
    "        FOR (e:Example) REQUIRE e.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "    kg.query(\"\"\"\n",
    "    CREATE CONSTRAINT unique_chunk_name IF NOT EXISTS \n",
    "        FOR (e:Chunk) REQUIRE e.name IS UNIQUE;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Query to delete the graph\n",
    "#kg.query(\"MATCH (n) DETACH DELETE n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Chunk {chunkSeqId: INTEGER, uuid: STRING, source: STRING, content: STRING, textEmbedding: LIST, name: STRING, type: STRING, creation_date: STRING, last_modified: STRING}\n",
      "Company {uuid: STRING, name: STRING, type: STRING, country: STRING, creation_date: STRING, last_modified: STRING, color: STRING}\n",
      "Concept {uuid: STRING, name: STRING, type: STRING, creation_date: STRING, last_modified: STRING, color: STRING, synonyms: LIST, description: STRING, descriptionEmbedding: LIST}\n",
      "Metric {uuid: STRING, name: STRING, type: STRING, creation_date: STRING, last_modified: STRING, color: STRING, description: STRING}\n",
      "Formula {uuid: STRING, name: STRING, type: STRING, creation_date: STRING, last_modified: STRING, color: STRING, formula: STRING}\n",
      "Rule {uuid: STRING, name: STRING, type: STRING, creation_date: STRING, last_modified: STRING, color: STRING, rule_description: STRING}\n",
      "Condition {uuid: STRING, name: STRING, type: STRING, creation_date: STRING, last_modified: STRING, color: STRING, condition: STRING}\n",
      "Example {uuid: STRING, name: STRING, type: STRING, creation_date: STRING, last_modified: STRING, color: STRING, example: STRING}\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "(:Chunk)-[:SOURCE]->(:Concept)\n",
      "(:Chunk)-[:SOURCE]->(:Example)\n",
      "(:Chunk)-[:NEXT]->(:Chunk)\n",
      "(:Company)-[:IS_RELATED_TO]->(:Company)\n",
      "(:Company)-[:IS_RELATED_TO]->(:Concept)\n",
      "(:Concept)-[:IS_RELATED_TO]->(:Formula)\n",
      "(:Concept)-[:IS_RELATED_TO]->(:Condition)\n",
      "(:Concept)-[:IS_RELATED_TO]->(:Metric)\n",
      "(:Concept)-[:IS_RELATED_TO]->(:Concept)\n",
      "(:Concept)-[:IS_RELATED_TO]->(:Rule)\n",
      "(:Concept)-[:USES_RULE]->(:Rule)\n",
      "(:Concept)-[:EXAMPLE]->(:Example)\n",
      "(:Formula)-[:FORMULA]->(:Concept)\n",
      "(:Formula)-[:FORMULA]->(:Formula)\n",
      "(:Rule)-[:USES_RULE]->(:Concept)\n",
      "(:Condition)-[:IS_CONDITIONED]->(:Condition)\n",
      "(:Example)-[:EXAMPLE]->(:Concept)\n"
     ]
    }
   ],
   "source": [
    "kg.refresh_schema()\n",
    "print(kg.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_file_path = '/data/expertai/expertai/knowledge/knowledgebase/expertai.txt'\n",
    "\n",
    "with open(roi_file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uuid():\n",
    "    letters_and_digits = string.ascii_letters + string.digits\n",
    "    def random_segment(length):\n",
    "        return ''.join(random.choice(letters_and_digits) for _ in range(length))\n",
    "    \n",
    "    uuid_like = f\"{random_segment(8)}-{random_segment(4)}-{random_segment(4)}-{random_segment(4)}-{random_segment(12)}\"\n",
    "    return uuid_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunk Documents\n",
    "def chunk_text(file, name):\n",
    "    \"\"\"Chunks and generate metadata for a txt file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        The content of the file\n",
    "    name : str\n",
    "        The name of the file\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    \"\"\"\n",
    "    chunks_with_metadata = [] #use this to accumulate chunk records\n",
    "    \n",
    "    #LangChain helper function for chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000, #2000 characters\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    "    ) #Antropic Contextual RAG sugiere Mario ContextualChunking\n",
    "\n",
    "    item_text_chunks = text_splitter.split_text(file) # split the text into chunks\n",
    "    chunk_seq_id = 0\n",
    "\n",
    "    for chunk in item_text_chunks: \n",
    "        # finally, construct a record with metadata and the chunk text\n",
    "        chunks_with_metadata.append({\n",
    "            'type': 'chunk',\n",
    "            'content': chunk, \n",
    "            # metadata from looping...\n",
    "            'chunkSeqId': chunk_seq_id, \n",
    "            'name' : f'{name}-chunk{chunk_seq_id:04d}', #To identify each unique chunk\n",
    "            # constructed metadata...\n",
    "            'uuid': str(generate_uuid()),\n",
    "            'creation_date': str(datetime.now()),\n",
    "            'last_modified': str(datetime.now()),\n",
    "            # metadata from file...\n",
    "            'source': f'{name}.txt',\n",
    "            })\n",
    "        chunk_seq_id += 1\n",
    "    print(f'\\tSplit into {chunk_seq_id} chunks')\n",
    "    return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSplit into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "#Change the filename on each iteration\n",
    "chunks = chunk_text(content, 'expertai') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_names = [chunk['name'] for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity expertai-chunk0000 of type chunk added to the graph\n"
     ]
    }
   ],
   "source": [
    "#Add each chunk node to the graph\n",
    "def add_chunks(chunks, kg):\n",
    "    \"\"\"\n",
    "    Add Chunk Nodes to Neo4J Knowledge Graph and Create NEXT Relationships.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunks : list\n",
    "        List of chunks with metadata\n",
    "    kg : str\n",
    "        Neo4j Driver\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Success string\n",
    "    \"\"\"\n",
    "    # Sort chunks based on chunkSeqId to ensure proper order\n",
    "    chunks = sorted(chunks, key=lambda x: x['chunkSeqId'])\n",
    "\n",
    "    previous_chunk_seqid = None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk_type = chunk['type']\n",
    "        chunk_name = chunk['name']\n",
    "\n",
    "        if chunk_type == 'chunk':\n",
    "\n",
    "            chunk_with_properties = {\n",
    "                'type': chunk_type,\n",
    "                'uuid': chunk['uuid'],\n",
    "                'name': chunk_name,\n",
    "                'content': chunk['content'],\n",
    "                'chunkSeqId': chunk['chunkSeqId'],\n",
    "                'creation_date': chunk['creation_date'],\n",
    "                'last_modified': chunk['last_modified'],\n",
    "                'source': chunk['source'],\n",
    "            }\n",
    "\n",
    "            merge_chunk_node_query = \"\"\"\n",
    "                MERGE(mergedChunk:Chunk {name: $chunkParam.name})\n",
    "                    ON CREATE SET \n",
    "                        mergedChunk.type = $chunkParam.type,\n",
    "                        mergedChunk.uuid = $chunkParam.uuid,\n",
    "                        mergedChunk.content = $chunkParam.content, \n",
    "                        mergedChunk.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "                        mergedChunk.creation_date = $chunkParam.creation_date, \n",
    "                        mergedChunk.last_modified = $chunkParam.last_modified,\n",
    "                        mergedChunk.source = $chunkParam.source\n",
    "                RETURN mergedChunk\n",
    "            \"\"\"\n",
    "            # Add the chunk node to the graph\n",
    "            kg.query(merge_chunk_node_query, params={'chunkParam': chunk_with_properties})\n",
    "\n",
    "            # Create a NEXT relationship if there is a previous chunk\n",
    "            if previous_chunk_seqid is not None:\n",
    "                create_next_relationship_query = \"\"\"\n",
    "                    MATCH (prevChunk:Chunk {chunkSeqId: $prevSeqid})\n",
    "                    MATCH (currentChunk:Chunk {chunkSeqId: $currentSeqid})\n",
    "                    WHERE prevChunk.source = currentChunk.source\n",
    "                    MERGE (prevChunk)-[:NEXT]->(currentChunk)\n",
    "                \"\"\"\n",
    "                kg.query(create_next_relationship_query, \n",
    "                         params={'prevSeqid': previous_chunk_seqid, 'currentSeqid': chunk['chunkSeqId']})\n",
    "\n",
    "            # Update previous_chunk_uuid to the current chunk's UUID\n",
    "            previous_chunk_seqid = chunk['chunkSeqId']\n",
    "\n",
    "        print(f'Entity {chunk_name} of type {chunk_type} added to the graph')\n",
    "\n",
    "add_chunks(chunks, kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_relationships(chunk_name, node_name, kg):\n",
    "    \"\"\"\n",
    "    Add Chunk Relationships to the Neo4J Knowledge Graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_name : str\n",
    "        The name of the chunk\n",
    "    node_name : str\n",
    "        The name of the Concept or Example node\n",
    "    kg : Neo4j\n",
    "        The connected Neo4j graph instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    source = chunk_name\n",
    "    target = node_name\n",
    "    rel_type = \"SOURCE\"\n",
    "\n",
    "        # Cypher query to create the relationship\n",
    "    create_relationship_query = f\"\"\"\n",
    "        MATCH (a {{name: $sourceName}})\n",
    "        MATCH (b {{name: $targetName}})\n",
    "        MERGE (a)-[r:{rel_type}]->(b)\n",
    "        RETURN r\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    kg.query(\n",
    "        create_relationship_query,\n",
    "        params={\n",
    "                'sourceName': source,\n",
    "                'targetName': target,\n",
    "        }\n",
    "    )\n",
    "    print(f\"Added relationship: ({source})-[:{rel_type}]->({target})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Entities with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entity_extractor(\n",
    "    chunk: str = \"\",\n",
    "    filename: str = \"\",\n",
    "    schema: str = \"\",\n",
    "    llm: \"RunnableChain\" = None,\n",
    ") -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    In JSON format outputs relevant entities and its properties.\n",
    "\n",
    "    Parameters\n",
    "    ----------.\n",
    "    chunk : str\n",
    "        Data of the chunk\n",
    "    filename : str\n",
    "        name of the source filename (Principal Entity)\n",
    "    schema : str\n",
    "        The actual schema of the knowledge graph\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        JSON of the recognized Entities\n",
    "    \"\"\"\n",
    "\n",
    "    if llm is None:\n",
    "        raise ValueError(\"llm must be provided\")\n",
    "\n",
    "    persona = (\n",
    "        \"\"\"You are an experienced node entity recognizer.\n",
    "        You will be provided with a chunk of data corresponding to a txt file.\n",
    "        Your taks is to recognize most relevant entities inside the chunk of data\n",
    "        based on the graph schema provided. After identifying the entities you must\n",
    "        fill the necessary properties for each node. \n",
    "\n",
    "        You must output ONLY the JSON file\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    context = (\n",
    "        f\"\"\"\n",
    "        <data chunk>\n",
    "        {chunk}\n",
    "        </data chunk>\n",
    "        \"\"\"\n",
    "        if chunk\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    best_practices = \"\"\"\n",
    "    Here’s a comprehensive list of rules for finding the node entities and populating its properties:\n",
    "\n",
    "    1. **The filename is the source document. Always make a concept entity with the same name.\n",
    "    2. ** Do not add information of data that is not inside the chunk of data.\n",
    "    3. ** When an example node is identified, add to its example property every step do not try to summarize the example.\n",
    "    4. ** For concept nodes if there are different synonyms in the chunk of data that refer to the same, use the most used as the name \n",
    "          property and the others\n",
    "          add them to the list of synonyms.\n",
    "    5. ** Never add node types not declared on the schema.\n",
    "    6. ** Dont change the language of data in the chunk, let it as it is. \n",
    "    7. ** At example nodes, include all steps explicitly as they appear in the chunk.\n",
    "    8. ** Each example must retain exact numerical values and computations.\n",
    "    9. ** Identify all explicit rules, including those prefaced by 'Always', 'Never', or 'Do not', and ensure they are included as `Rule` nodes.\n",
    "    10. ** Extract all explicit and implicit rules mentioned in the chunk of data.\n",
    " \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "        <role>\n",
    "        {ROLE}\n",
    "        </role>\n",
    "        <best_practices>\n",
    "        {BEST_PRACTICES}\n",
    "        </best_practices>\n",
    "        <source>\n",
    "        {FILENAME}\n",
    "        </source>\n",
    "        {context}\n",
    "        <schema>\n",
    "        {SCHEMA}\n",
    "        </schema>\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=user_prompt,\n",
    "        input_variables=[\"context\"],\n",
    "        partial_variables={\n",
    "            \"BEST_PRACTICES\": best_practices,\n",
    "            \"ROLE\": persona,\n",
    "            \"FILENAME\": filename,\n",
    "            \"SCHEMA\": schema,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "        \n",
    "    response = chain.invoke({\"context\": context}).content.strip()\n",
    "\n",
    "    # Validate and format JSON\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        return json.dumps(json_response, indent=4)  # Prettify JSON output\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The LLM response is not valid JSON.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "Person {\n",
    "    type: STRING,\n",
    "    name: STRING,\n",
    "    occupation: STRING\n",
    "}\n",
    "\n",
    "Company {\n",
    "    type: STRING,\n",
    "    name: STRING,\n",
    "    description: STRING\n",
    "}\n",
    "\n",
    "Concept {\n",
    "    type: STRING,\n",
    "    name: STRING,\n",
    "    synonyms: LIST,\n",
    "    description: STRING\n",
    "}\n",
    "\n",
    "Metric {\n",
    "    type: STRING,\n",
    "    name: STRING,\n",
    "    description: STRING\n",
    "}\n",
    "\n",
    "Formula {\n",
    "    type: STRING,\n",
    "    name: STRING,\n",
    "    formula: STRING\n",
    "}\n",
    "\n",
    "Rule {\n",
    "    type: STRING,\n",
    "    name: STRING,\n",
    "    rule_description: STRING\n",
    "}\n",
    "\n",
    "Condition {\n",
    "    type: STRING,\n",
    "    name: STRING,\n",
    "    condition: STRING\n",
    "}\n",
    "\n",
    "Example {\n",
    "    type: STRING,\n",
    "    name: STRING,\n",
    "    example: STRING\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = Entity_extractor(content, \"expertai.txt\", schema, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"nodes\": [\n",
      "        {\n",
      "            \"type\": \"Concept\",\n",
      "            \"name\": \"expertai\",\n",
      "            \"synonyms\": [\n",
      "                \"asistente experto de Kuona\"\n",
      "            ],\n",
      "            \"description\": \"Nueva herramienta de la familia Kuona especializada en an\\u00e1lisis de promociones y revenue management\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Company\",\n",
      "            \"name\": \"Kuona\",\n",
      "            \"description\": \"Empresa que desarrolla herramientas de an\\u00e1lisis de promociones y revenue management\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Company\",\n",
      "            \"name\": \"Arca Continental M\\u00e9xico\",\n",
      "            \"description\": \"Cliente inicial del asistente experto de Kuona\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Metric\",\n",
      "            \"name\": \"ROI\",\n",
      "            \"description\": \"Retorno de inversi\\u00f3n en an\\u00e1lisis de promociones\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Concept\",\n",
      "            \"name\": \"elasticidad\",\n",
      "            \"synonyms\": [],\n",
      "            \"description\": \"M\\u00e9trica de an\\u00e1lisis en promociones\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Concept\",\n",
      "            \"name\": \"canibalizaci\\u00f3n\",\n",
      "            \"synonyms\": [],\n",
      "            \"description\": \"M\\u00e9trica de an\\u00e1lisis en promociones\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Concept\",\n",
      "            \"name\": \"vol\\u00famenes incrementales\",\n",
      "            \"synonyms\": [],\n",
      "            \"description\": \"M\\u00e9trica de an\\u00e1lisis en promociones\"\n",
      "        },\n",
      "        {\n",
      "            \"type\": \"Concept\",\n",
      "            \"name\": \"revenue management\",\n",
      "            \"synonyms\": [],\n",
      "            \"description\": \"\\u00c1rea de especializaci\\u00f3n del asistente experto\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relationship_extractor(\n",
    "    Entities: str = \"\",\n",
    "    filename: str = \"\",\n",
    "    schema: str = \"\",\n",
    "    llm: \"RunnableChain\" = None,\n",
    ") -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    In JSON format outputs relevant relationships between entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------.\n",
    "    Entities : str\n",
    "        Entities extracted form a chunk of data\n",
    "    filename : str\n",
    "        name of the source filename (Principal Entity)\n",
    "    schema : str\n",
    "        The actual schema of the knowledge graph\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        JSON of relationships \n",
    "    \"\"\"\n",
    "\n",
    "    if llm is None:\n",
    "        raise ValueError(\"llm must be provided\")\n",
    "\n",
    "    persona = (\n",
    "        \"\"\"You are an experienced Entity relationships recognizer.\n",
    "        You will be provided with entities extracted form a chunk of data corresponding to a txt file.\n",
    "        Your taks is to recognize relevant relationships between this entities\n",
    "        based on the graph schema provided. After identifying the relationships \n",
    "        You must output ONLY the JSON file\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    context = (\n",
    "        f\"\"\"\n",
    "        <Entities>\n",
    "        {Entities}\n",
    "        </Entities>\n",
    "        \"\"\"\n",
    "        if Entities\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    best_practices = \"\"\"\n",
    "    Here’s a comprehensive list of rules for finding the nodes relationships:\n",
    "\n",
    "    1. ** Use only relationships defined in the schema.\n",
    "    2. ** Dont add any relationship that is NOT DEFINED in schema.\n",
    "    3. ** Source and Target nodes must be only the ones defined in the Entities context.\n",
    "    4. ** To identify the nodes use only the \"name\" property of each one \n",
    "    5. ** An entity can't Relate back to itself\n",
    "    6. ** A Source entity do not have to relate to a Destiny entity with the same name.\n",
    "    7. ** Source and Destiny entities are defines on the SCHEMA. Do Not Mix them. \n",
    "    8. ** Example and Rule entity nodes must be related at least to one concept node.\n",
    "\n",
    "    Example of a good output:\n",
    "\n",
    "    {\n",
    "    \"relationships\": [\n",
    "        {\n",
    "            \"source\": \"Arca Continental\",\n",
    "            \"type\": \"IS_RELATED_TO\",\n",
    "            \"target\": \"Arca Mexico\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"Arca Mexico\",\n",
    "            \"type\": \"IS_RELATED_TO\",\n",
    "            \"target\": \"The Coca-Cola Company\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"sellin\",\n",
    "            \"type\": \"IS_RELATED_TO\",\n",
    "            \"target\": \"traditional channel\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"sellin\",\n",
    "            \"type\": \"IS_RELATED_TO\",\n",
    "            \"target\": \"client\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"Use sellin not sellout\",\n",
    "            \"type\": \"USES_RULE\",\n",
    "            \"target\": \"sellin\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "        <role>\n",
    "        {ROLE}\n",
    "        </role>\n",
    "        <best_practices>\n",
    "        {BEST_PRACTICES}\n",
    "        </best_practices>\n",
    "        <source>\n",
    "        {FILENAME}\n",
    "        </source>\n",
    "        {context}\n",
    "        <schema>\n",
    "        {SCHEMA}\n",
    "        </schema>\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=user_prompt,\n",
    "        input_variables=[\"context\"],\n",
    "        partial_variables={\n",
    "            \"BEST_PRACTICES\": best_practices,\n",
    "            \"ROLE\": persona,\n",
    "            \"FILENAME\": filename,\n",
    "            \"SCHEMA\": schema,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "        \n",
    "    response = chain.invoke({\"context\": context}).content.strip()\n",
    "\n",
    "    # Validate and format JSON\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        return json.dumps(json_response, indent=4)  # Prettify JSON output\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(\"The LLM response is not valid JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "\n",
    "The relationships are the following:\n",
    "\n",
    "(:Person)-[:WORKS_AT]->(:Company),\n",
    "(:Company)-[:IS_RELATED_TO]->(:Company),\n",
    "(:Company)-[:IS_RELATED_TO]->(:Concept),\n",
    "(:Concept)-[:IS_RELATED_TO]->(:Concept),\n",
    "(:Concept)-[:IS_RELATED_TO]->(:Metric),\n",
    "(:Metric)-[:FORMULA]->(:Formula),\n",
    "(:Concept)-[:USES_RULE]->(:Rule),\n",
    "(:Rule)-[:IS_CONDITIONED]->(:Condition),\n",
    "(:Concept)-[:EXAMPLE]->(:Example),\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = Relationship_extractor(nodes, \"expertai.txt\", schema, llm=llm_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"relationships\": [\\n        {\\n            \"source\": \"Kuona\",\\n            \"type\": \"IS_RELATED_TO\",\\n            \"target\": \"expertai\"\\n        },\\n        {\\n            \"source\": \"Kuona\",\\n            \"type\": \"IS_RELATED_TO\",\\n            \"target\": \"revenue management\"\\n        },\\n        {\\n            \"source\": \"expertai\",\\n            \"type\": \"IS_RELATED_TO\",\\n            \"target\": \"Arca Continental M\\\\u00e9xico\"\\n        },\\n        {\\n            \"source\": \"expertai\",\\n            \"type\": \"IS_RELATED_TO\",\\n            \"target\": \"revenue management\"\\n        },\\n        {\\n            \"source\": \"revenue management\",\\n            \"type\": \"IS_RELATED_TO\",\\n            \"target\": \"elasticidad\"\\n        },\\n        {\\n            \"source\": \"revenue management\",\\n            \"type\": \"IS_RELATED_TO\",\\n            \"target\": \"canibalizaci\\\\u00f3n\"\\n        },\\n        {\\n            \"source\": \"revenue management\",\\n            \"type\": \"IS_RELATED_TO\",\\n            \"target\": \"vol\\\\u00famenes incrementales\"\\n        },\\n        {\\n            \"source\": \"revenue management\",\\n            \"type\": \"IS_RELATED_TO\",\\n            \"target\": \"ROI\"\\n        }\\n    ]\\n}'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_relationships(relationships, original_node, new_node):\n",
    "    relationships = json.loads(relationships)\n",
    "    for relationship in relationships['relationships']:\n",
    "        if relationship[\"source\"] == original_node:\n",
    "            relationship[\"source\"] = new_node\n",
    "        if relationship[\"target\"] == original_node:\n",
    "            relationship[\"target\"] = new_node\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Entities and Relationships to the Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to JSON\n",
    "nodes = json.loads(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Concept', 'name': 'expertai', 'synonyms': ['asistente experto de Kuona'], 'description': 'Nueva herramienta de la familia Kuona especializada en análisis de promociones y revenue management'}\n",
      "Concept\n",
      "{'type': 'Company', 'name': 'Kuona', 'description': 'Empresa que desarrolla herramientas de análisis de promociones y revenue management'}\n",
      "Company\n",
      "{'type': 'Company', 'name': 'Arca Continental México', 'description': 'Cliente inicial del asistente experto de Kuona'}\n",
      "Company\n",
      "{'type': 'Metric', 'name': 'ROI', 'description': 'Retorno de inversión en análisis de promociones'}\n",
      "Metric\n",
      "{'type': 'Concept', 'name': 'elasticidad', 'synonyms': [], 'description': 'Métrica de análisis en promociones'}\n",
      "Concept\n",
      "{'type': 'Concept', 'name': 'canibalización', 'synonyms': [], 'description': 'Métrica de análisis en promociones'}\n",
      "Concept\n",
      "{'type': 'Concept', 'name': 'volúmenes incrementales', 'synonyms': [], 'description': 'Métrica de análisis en promociones'}\n",
      "Concept\n",
      "{'type': 'Concept', 'name': 'revenue management', 'synonyms': [], 'description': 'Área de especialización del asistente experto'}\n",
      "Concept\n"
     ]
    }
   ],
   "source": [
    "# Now you can iterate over `json_data`\n",
    "for node in nodes[\"nodes\"]:\n",
    "    print(node)\n",
    "    node_type = node['type']\n",
    "    print(node_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deduplication_query(node_name, node_description):\n",
    "    similarity_threshold = 0.9\n",
    "    word_edit_distance = 5\n",
    "\n",
    "\n",
    "    data = kg.query(\"\"\"\n",
    "    WITH genai.vector.encode($description, \"OpenAI\", {token: $openAiApiKey}) AS description_embedding\n",
    "\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, description_embedding)\n",
    "    YIELD node, score\n",
    "\n",
    "    WHERE score > toFloat($cutoff)\n",
    "    AND (\n",
    "        toLower(node.name) CONTAINS toLower($name)\n",
    "        OR toLower($name) CONTAINS toLower(node.name)\n",
    "        OR apoc.text.distance(toLower(node.name), toLower($name)) < $distance\n",
    "    )\n",
    "\n",
    "    RETURN node.name, node.description, score\n",
    "    ORDER BY score DESC;\n",
    "\n",
    "    \"\"\", params={'name': node_name,\n",
    "                'description': node_description, \n",
    "                'openAiApiKey':OPENAI_API_KEY,\n",
    "                'index_name':'vector_concepts', \n",
    "                'top_k': 3,\n",
    "                'cutoff': similarity_threshold, \n",
    "                'distance': word_edit_distance})\n",
    "    \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplication_llm(\n",
    "    new_concept: str = \"\",\n",
    "    similar_concepts: list = [],\n",
    "    llm: \"RunnableChain\" = None,\n",
    ") -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    LLM compares a list of very similar concept nodes with a new concept node identified previously.\n",
    "\n",
    "    If LLM identifies new concept node as unique it will output the same name as it is.\n",
    "\n",
    "    If LLM identifies a existing concept node as the same as new concept node, \n",
    "    new concept node name will be added to the list of synonyms of the existing concept node and \n",
    "    the output will be the name of the existing concept node.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------.\n",
    "    new_concept : str\n",
    "        New Concept Entity identified \n",
    "    similar_concepts : list\n",
    "        list of similar existing concept nodes previously extracted using vector search and Lebenshtein\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Name of the concept node \n",
    "    \"\"\"\n",
    "\n",
    "    if llm is None:\n",
    "        raise ValueError(\"llm must be provided\")\n",
    "\n",
    "    persona = (\n",
    "        \"\"\"You are an experienced Entity Resolution recognizer.\n",
    "        You will be provided with a new concept extracted previously from a document,\n",
    "        and with a list of very similar concepts extracted from the knowledge graph.\n",
    "        Based on name and the given description of each one you will define if the\n",
    "        new concept provided is really a unique concept or if it should be added to\n",
    "        the synonym list of an existing concept node. \n",
    "        \n",
    "        After deciding You must output \n",
    "        ONLY the NAME of the concept.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    context = (\n",
    "        f\"\"\"\n",
    "        <List of Similar Existing Concept Nodes>\n",
    "        {similar_concepts}\n",
    "        </List of Similar Existing Concept Nodes>\n",
    "        \"\"\"\n",
    "        if similar_concepts\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    best_practices = \"\"\"\n",
    "    Here’s a comprehensive list of rules for deciding:\n",
    "\n",
    "    1. **  If you identify the new concept node as unique you will output the same name of the concept as it is.\n",
    "    2. ** If you identify a existing concept node as the same as new concept node, \n",
    "    new concept node name will be added to the list of synonyms of the existing concept node and \n",
    "    the output will be the name of the existing concept node.\n",
    "    3. ** Take careful consideration on the description of each concept, and if they talk about the same concept.\n",
    "    4. ** If the list of similar existing concept nodes is empty, then the output must be the name of the new concept.\n",
    "    5. ** ONLY OUTPUT THE NAME OF THE CONCEPT NODE DONT EXPLAIN THE REASONING\n",
    "\n",
    "    Example of a good output:\n",
    "\n",
    "    <new_concept>\n",
    "       node.name: Impacto de Canibalización, \n",
    "       node.description: El impacto de canibalización mide la pérdida causada por la disminución en ventas de otros \n",
    "       productos debido a una promoción. Representa lo que se dejó de percibir de productos no promocionados cuando los \n",
    "       clientes optaron por comprar el producto en promoción.\n",
    "    </new_concept>\n",
    "\n",
    "    <List of Similar Existing Concept Nodes>\n",
    "\n",
    "    [{'node.name': 'Impacto de Canibalización', \n",
    "      'node.description': 'El impacto de canibalización mide la pérdida causada por la disminución en ventas de otros \n",
    "      productos debido a una promoción. Representa lo que se dejó de percibir de productos no promocionados cuando los \n",
    "      clientes optaron por comprar el producto en promoción.',\n",
    "      'score': 0.996490478515625}, \n",
    "     {'node.name': 'Canibalización',\n",
    "      'node.description': \"La canibalización en el contexto de una promoción se refiere al fenómeno en el que una acción \n",
    "      promocional para un producto o servicio afecta negativamente las ventas de otros productos o servicios de la misma empresa. \n",
    "      En otras palabras, es cuando una promoción 'se come' las ventas de otros productos.\", \n",
    "      'score': 0.9651641845703125}]\n",
    "\n",
    "    </List of Similar Existing Concept Nodes>\n",
    "\n",
    "    EXPECTED OUTPUT:\n",
    "\n",
    "    Impacto de Canibalización\n",
    "\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "        <role>\n",
    "        {ROLE}\n",
    "        </role>\n",
    "        <best_practices>\n",
    "        {BEST_PRACTICES}\n",
    "        </best_practices>\n",
    "        <new_concept>\n",
    "        {NEW_CONCEPT}\n",
    "        </new_concept>\n",
    "        {context}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=user_prompt,\n",
    "        input_variables=[\"context\"],\n",
    "        partial_variables={\n",
    "            \"BEST_PRACTICES\": best_practices,\n",
    "            \"ROLE\": persona,\n",
    "            \"NEW_CONCEPT\": new_concept,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "        \n",
    "    response = chain.invoke({\"context\": context}).content.strip()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_synonym(concept, synonym, kg):\n",
    "    change = kg.query(\"\"\"\n",
    "    WITH $synonym_to_add AS newSynonym\n",
    "    MATCH (c:Concept {name: $concept_to_update})\n",
    "        SET c.synonyms = CASE \n",
    "        WHEN c.synonyms IS NULL THEN [newSynonym] \n",
    "        ELSE apoc.coll.union(c.synonyms, [newSynonym]) \n",
    "    END\n",
    "    RETURN c\n",
    "    \"\"\", params={'synonym_to_add': synonym,\n",
    "                'concept_to_update': concept})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_description(concept, addition, kg):\n",
    "    change = kg.query(\"\"\"\n",
    "    WITH $string_to_add AS addition\n",
    "    MATCH (c:Concept {name: $concept_to_update})\n",
    "        SET c.description = CASE \n",
    "        WHEN c.description IS NULL THEN addition \n",
    "        ELSE c.description + ' ' + addition \n",
    "    END\n",
    "    RETURN c\n",
    "    \"\"\", params={'string_to_add': addition,\n",
    "                 'concept_to_update': concept})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Added relationship: (expertai-chunk0000)-[:SOURCE]->(expertai)\n",
      "Entity expertai of type Concept added to the graph\n",
      "Entity Kuona of type Company added to the graph\n",
      "Entity Arca Continental México of type Company added to the graph\n",
      "Entity ROI of type Metric added to the graph\n",
      "[]\n",
      "Added relationship: (expertai-chunk0000)-[:SOURCE]->(elasticidad)\n",
      "Entity elasticidad of type Concept added to the graph\n",
      "[{'node.name': 'Impacto de Canibalización', 'node.description': 'El impacto de canibalización mide la pérdida causada por la disminución en ventas de otros productos debido a una promoción. Representa lo que se dejó de percibir de productos no promocionados cuando los clientes optaron por comprar el producto en promoción.', 'score': 0.91766357421875}]\n",
      "Entity canibalización identified as synonym of Entity Impacto de Canibalización\n",
      "Added relationship: (expertai-chunk0000)-[:SOURCE]->(Impacto de Canibalización)\n",
      "Entity Impacto de Canibalización of type Concept added to the graph\n",
      "[]\n",
      "Added relationship: (expertai-chunk0000)-[:SOURCE]->(volúmenes incrementales)\n",
      "Entity volúmenes incrementales of type Concept added to the graph\n",
      "[]\n",
      "Added relationship: (expertai-chunk0000)-[:SOURCE]->(revenue management)\n",
      "Entity revenue management of type Concept added to the graph\n"
     ]
    }
   ],
   "source": [
    "def add_entities(nodes, kg, chunks, relationships):\n",
    "    \"\"\"\n",
    "    Add Node Entities to Neo4J Knowledge Graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : json\n",
    "        Entities extracted form a chunk of data\n",
    "    kg : Neo4J\n",
    "        Neo4J driver\n",
    "    chunk: List\n",
    "        List of chunk names retrieved from document\n",
    "    relationships_ dict\n",
    "        Json of the proposed relationships for the nodes\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Success string\n",
    "    \"\"\"\n",
    "\n",
    "    for node in nodes['nodes']:\n",
    "\n",
    "        node_type = node['type']\n",
    "        node_name = node['name']\n",
    "\n",
    "        if node_type == 'Person':\n",
    "\n",
    "            node_with_properties = {\n",
    "                'type': node_type,\n",
    "                'uuid': generate_uuid(),\n",
    "                'name': node['name'],\n",
    "                'occupation': node['occupation'],\n",
    "                'creation_date': str(datetime.now()),\n",
    "                'last_modified': str(datetime.now()),\n",
    "                'color': 'red',\n",
    "            }\n",
    "\n",
    "            merge_person_node_query = \"\"\"\n",
    "                MERGE(mergedPerson:Person {name: $personParam.name})\n",
    "                    ON CREATE SET \n",
    "                        mergedPerson.type = $personParam.type,\n",
    "                        mergedPerson.uuid = $personParam.uuid,\n",
    "                        mergedPerson.occupation = $personParam.occupation, \n",
    "                        mergedPerson.creation_date = $personParam.creation_date, \n",
    "                        mergedPerson.last_modified = $personParam.last_modified,  \n",
    "                        mergedPerson.color = $personParam.color \n",
    "                RETURN mergedPerson\n",
    "                \"\"\"\n",
    "            #Only adding one node\n",
    "            kg.query(merge_person_node_query, \n",
    "                    params={'personParam':node_with_properties})\n",
    "        \n",
    "        \n",
    "        if node_type == 'Company':\n",
    "\n",
    "            node_with_properties = {\n",
    "                'type': node_type,\n",
    "                'uuid': generate_uuid(),\n",
    "                'name': node['name'],\n",
    "                'country': 'Mexico',\n",
    "                'description': node['description'],\n",
    "                'creation_date': str(datetime.now()),\n",
    "                'last_modified': str(datetime.now()),\n",
    "                'color': 'orange',\n",
    "            }\n",
    "\n",
    "            merge_company_node_query = \"\"\"\n",
    "            MERGE(mergedCompany:Company {name: $companyParam.name})\n",
    "                ON CREATE SET \n",
    "                    mergedCompany.type = $companyParam.type,\n",
    "                    mergedCompany.uuid = $companyParam.uuid,\n",
    "                    mergedCompany.country = $companyParam.country, \n",
    "                    mergedCompany.description = $companyParam.description, \n",
    "                    mergedCompany.creation_date = $companyParam.creation_date, \n",
    "                    mergedCompany.last_modified = $companyParam.last_modified,  \n",
    "                    mergedCompany.color = $companyParam.color \n",
    "            RETURN mergedCompany\n",
    "            \"\"\"\n",
    "\n",
    "            #Only adding one node\n",
    "            kg.query(merge_company_node_query, \n",
    "                    params={'companyParam':node_with_properties})\n",
    "            \n",
    "\n",
    "        #Check for conept entity the vector Embedding property need to be applied \n",
    "        if node_type == 'Concept':\n",
    "            node_description = node['description']\n",
    "\n",
    "            similar_concepts = deduplication_query(node_name, node_description) #Get similar concept nodes from KG\n",
    "            print(similar_concepts)\n",
    "\n",
    "            concept = deduplication_llm(node, similar_concepts, llm=llm) #Outputs name of final concept node\n",
    "\n",
    "            if concept != node_name:\n",
    "\n",
    "                relationships = modify_relationships(relationships, node_name, concept) #Modify relationships\n",
    "                add_synonym(concept, node_name, kg) #Add node name to the list of synonyms of the concept node\n",
    "                add_to_description(concept, node_description, kg) #Concatenates New description to the existing concept\n",
    "                #Pending to update the embedding\n",
    "\n",
    "                print(f'Entity {node_name} identified as synonym of Entity {concept}')\n",
    "\n",
    "                node_name = concept #For relating chunks with existing concept\n",
    "                \n",
    "            else:\n",
    "                node_with_properties = {\n",
    "                'type': node_type, \n",
    "                'uuid': generate_uuid(),\n",
    "                'name': node['name'],\n",
    "                'synonyms' : node['synonyms'], #Check if this is correct for a list type\n",
    "                'description': node['description'],\n",
    "                # constructed metadata...\n",
    "                'creation_date': str(datetime.now()),\n",
    "                'last_modified': str(datetime.now()),\n",
    "                'color': 'blue',\n",
    "                }\n",
    "\n",
    "                merge_concept_node_query = \"\"\"\n",
    "                MERGE(mergedConcept:Concept {name: $conceptParam.name})\n",
    "                    ON CREATE SET \n",
    "                        mergedConcept.type = $conceptParam.type,\n",
    "                        mergedConcept.uuid = $conceptParam.uuid,\n",
    "                        mergedConcept.synonyms = $conceptParam.synonyms, \n",
    "                        mergedConcept.description = $conceptParam.description, \n",
    "                        mergedConcept.creation_date = $conceptParam.creation_date, \n",
    "                        mergedConcept.last_modified = $conceptParam.last_modified,  \n",
    "                        mergedConcept.color = $conceptParam.color \n",
    "                RETURN mergedConcept\n",
    "                \"\"\"\n",
    "\n",
    "                #Only adding one node\n",
    "                kg.query(merge_concept_node_query, \n",
    "                        params={'conceptParam':node_with_properties})\n",
    "            \n",
    "            #Add \"SOURCE\" Relationship to each corresponding chunk (Will probably be changed this on the future)\n",
    "            for chunk in chunks:\n",
    "                chunk_relationships(chunk, node_name, kg)\n",
    "\n",
    "    \n",
    "        if node_type == 'Metric':\n",
    "            node_with_properties = {\n",
    "            'type': node_type, \n",
    "            'uuid': generate_uuid(),\n",
    "            'name': node['name'],\n",
    "            'description': node['description'],\n",
    "            # constructed metadata...\n",
    "            'creation_date': str(datetime.now()),\n",
    "            'last_modified': str(datetime.now()),\n",
    "            'color': 'purple',\n",
    "            }\n",
    "\n",
    "            merge_metric_node_query = \"\"\"\n",
    "            MERGE(mergedMetric:Metric {name: $metricParam.name})\n",
    "                ON CREATE SET \n",
    "                    mergedMetric.type = $metricParam.type,\n",
    "                    mergedMetric.uuid = $metricParam.uuid,\n",
    "                    mergedMetric.description = $metricParam.description, \n",
    "                    mergedMetric.creation_date = $metricParam.creation_date, \n",
    "                    mergedMetric.last_modified = $metricParam.last_modified,  \n",
    "                    mergedMetric.color = $metricParam.color \n",
    "            RETURN mergedMetric\n",
    "            \"\"\"\n",
    "\n",
    "            #Only adding one node\n",
    "            kg.query(merge_metric_node_query, \n",
    "                    params={'metricParam':node_with_properties})\n",
    "\n",
    "        if node_type == 'Formula':\n",
    "            node_with_properties = {\n",
    "            'type': node_type, \n",
    "            'uuid': generate_uuid(),\n",
    "            'name': node['name'],\n",
    "            'formula' : node['formula'], \n",
    "            # constructed metadata...\n",
    "            'creation_date': str(datetime.now()),\n",
    "            'last_modified': str(datetime.now()),\n",
    "            'color': 'green',\n",
    "            }\n",
    "\n",
    "\n",
    "            merge_formula_node_query = \"\"\"\n",
    "            MERGE(mergedFormula:Formula {name: $formulaParam.name})\n",
    "                ON CREATE SET \n",
    "                    mergedFormula.type = $formulaParam.type,\n",
    "                    mergedFormula.uuid = $formulaParam.uuid,\n",
    "                    mergedFormula.formula = $formulaParam.formula, \n",
    "                    mergedFormula.creation_date = $formulaParam.creation_date, \n",
    "                    mergedFormula.last_modified = $formulaParam.last_modified,  \n",
    "                    mergedFormula.color = $formulaParam.color \n",
    "            RETURN mergedFormula\n",
    "            \"\"\"\n",
    "\n",
    "            #Only adding one node\n",
    "            kg.query(merge_formula_node_query, \n",
    "                    params={'formulaParam':node_with_properties})\n",
    "        \n",
    "        if node_type == 'Rule':\n",
    "            node_with_properties = {\n",
    "            'type': node_type, \n",
    "            'uuid': generate_uuid(),\n",
    "            'name': node['name'],\n",
    "            'rule_description' : node['rule_description'], \n",
    "            # constructed metadata...\n",
    "            'creation_date': str(datetime.now()),\n",
    "            'last_modified': str(datetime.now()),\n",
    "            'color': 'dark green',\n",
    "            }\n",
    "\n",
    "            merge_rule_node_query = \"\"\"\n",
    "            MERGE(mergedRule:Rule {name: $ruleParam.name})\n",
    "                ON CREATE SET \n",
    "                    mergedRule.type = $ruleParam.type,\n",
    "                    mergedRule.uuid = $ruleParam.uuid,\n",
    "                    mergedRule.rule_description = $ruleParam.rule_description, \n",
    "                    mergedRule.creation_date = $ruleParam.creation_date, \n",
    "                    mergedRule.last_modified = $ruleParam.last_modified,  \n",
    "                    mergedRule.color = $ruleParam.color \n",
    "            RETURN mergedRule\n",
    "            \"\"\"\n",
    "\n",
    "            #Only adding one node\n",
    "            kg.query(merge_rule_node_query, \n",
    "                    params={'ruleParam':node_with_properties})\n",
    "\n",
    "        if node_type == 'Condition':\n",
    "            node_with_properties = {\n",
    "            'type': node_type, \n",
    "            'uuid': generate_uuid(),\n",
    "            'name': node['name'],\n",
    "            'condition' : node['condition'], \n",
    "            # constructed metadata...\n",
    "            'creation_date': str(datetime.now()),\n",
    "            'last_modified': str(datetime.now()),\n",
    "            'color': 'green',\n",
    "            }\n",
    "\n",
    "            merge_condition_node_query = \"\"\"\n",
    "            MERGE(mergedCondition:Condition {name: $conditionParam.name})\n",
    "                ON CREATE SET \n",
    "                    mergedCondition.type = $conditionParam.type,\n",
    "                    mergedCondition.uuid = $conditionParam.uuid,\n",
    "                    mergedCondition.condition = $conditionParam.condition, \n",
    "                    mergedCondition.creation_date = $conditionParam.creation_date, \n",
    "                    mergedCondition.last_modified = $conditionParam.last_modified,  \n",
    "                    mergedCondition.color = $conditionParam.color \n",
    "            RETURN mergedCondition\n",
    "            \"\"\"\n",
    "\n",
    "            #Only adding one node\n",
    "            kg.query(merge_condition_node_query, \n",
    "                    params={'conditionParam':node_with_properties})\n",
    "\n",
    "        if node_type == 'Example':\n",
    "            node_with_properties = {\n",
    "            'type': node_type, \n",
    "            'uuid': generate_uuid(),\n",
    "            'name': node['name'],\n",
    "            'example' : node['example'], \n",
    "            # constructed metadata...\n",
    "            'creation_date': str(datetime.now()),\n",
    "            'last_modified': str(datetime.now()),\n",
    "            'color': 'green',\n",
    "            }\n",
    "\n",
    "            merge_example_node_query = \"\"\"\n",
    "            MERGE(mergedExample:Example {name: $exampleParam.name})\n",
    "                ON CREATE SET \n",
    "                    mergedExample.type = $exampleParam.type,\n",
    "                    mergedExample.uuid = $exampleParam.uuid,\n",
    "                    mergedExample.example = $exampleParam.example, \n",
    "                    mergedExample.creation_date = $exampleParam.creation_date, \n",
    "                    mergedExample.last_modified = $exampleParam.last_modified,  \n",
    "                    mergedExample.color = $exampleParam.color \n",
    "            RETURN mergedExample\n",
    "            \"\"\"\n",
    "\n",
    "            #Only adding one node\n",
    "            kg.query(merge_example_node_query, \n",
    "                    params={'exampleParam':node_with_properties})\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                chunk_relationships(chunk, node_name, kg)\n",
    "\n",
    "        \n",
    "        print(f'Entity {node_name} of type {node_type} added to the graph')\n",
    "       \n",
    "\n",
    "add_entities(nodes, kg, chunk_names, relationships)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to JSON\n",
    "relationships = json.loads(relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relationships(relationships, kg):\n",
    "    \"\"\"\n",
    "    Add Relationships to the Neo4J Knowledge Graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    relationships : dict\n",
    "        A dictionary containing the relationships to be added, \n",
    "        with each relationship defined by source, type, and target.\n",
    "    kg : Neo4j\n",
    "        The connected Neo4j graph instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for relationship in relationships['relationships']:\n",
    "        source = relationship['source']\n",
    "        target = relationship['target']\n",
    "        rel_type = relationship['type']\n",
    "\n",
    "        # Cypher query to create the relationship\n",
    "        create_relationship_query = f\"\"\"\n",
    "        MATCH (a {{name: $sourceName}})\n",
    "        MATCH (b {{name: $targetName}})\n",
    "        MERGE (a)-[r:{rel_type}]->(b)\n",
    "        RETURN r\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the query\n",
    "        kg.query(\n",
    "            create_relationship_query,\n",
    "            params={\n",
    "                'sourceName': source,\n",
    "                'targetName': target,\n",
    "            }\n",
    "        )\n",
    "        print(f\"Added relationship: ({source})-[:{rel_type}]->({target})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added relationship: (Kuona)-[:IS_RELATED_TO]->(expertai)\n",
      "Added relationship: (Kuona)-[:IS_RELATED_TO]->(revenue management)\n",
      "Added relationship: (expertai)-[:IS_RELATED_TO]->(Arca Continental México)\n",
      "Added relationship: (expertai)-[:IS_RELATED_TO]->(revenue management)\n",
      "Added relationship: (revenue management)-[:IS_RELATED_TO]->(elasticidad)\n",
      "Added relationship: (revenue management)-[:IS_RELATED_TO]->(canibalización)\n",
      "Added relationship: (revenue management)-[:IS_RELATED_TO]->(volúmenes incrementales)\n",
      "Added relationship: (revenue management)-[:IS_RELATED_TO]->(ROI)\n"
     ]
    }
   ],
   "source": [
    "add_relationships(relationships, kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tasks: \n",
    "#Generate the chunk nodes for each document - Complete\n",
    "#Each Chunk node should be related to Concept and Example nodes - Complete\n",
    "#Concept Nodes and Chunk Nodes should have a Vector embedding property for vector search - Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "VECTOR_INDEX_NAME = 'vector_chunks'\n",
    "VECTOR_NODE_LABEL = 'Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY = 'textEmbedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vector Index is called vector_chunks and will store embeddings for nodes labeled as Chunk in a property called textEmbedding\n",
    "#Configuration is based on OpenAI Embedding Model\n",
    "kg.query(\"\"\"\n",
    "         CREATE VECTOR INDEX `vector_chunks` IF NOT EXISTS\n",
    "          FOR (c:Chunk) ON (c.textEmbedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 1536,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add Embedding Property to the Nodes\n",
    "\n",
    "kg.query(\"\"\"\n",
    "    MATCH (chunk:Chunk) WHERE chunk.textEmbedding IS NULL\n",
    "    WITH chunk, genai.vector.encode(\n",
    "      chunk.content, \n",
    "      \"OpenAI\", \n",
    "      {\n",
    "        token: $openAiApiKey\n",
    "      }) AS vector\n",
    "    CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", vector)\n",
    "    \"\"\", \n",
    "    params={\"openAiApiKey\":OPENAI_API_KEY} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vector Index is called vector_concepts and will store embeddings for nodes labeled as Concept in a property called descriptionEmbedding\n",
    "#Configuration is based on OpenAI Embedding Model\n",
    "kg.query(\"\"\"\n",
    "         CREATE VECTOR INDEX `vector_concepts` IF NOT EXISTS\n",
    "          FOR (c:Concept) ON (c.descriptionEmbedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 1536,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add Embedding Property to the Nodes\n",
    "\n",
    "kg.query(\"\"\"\n",
    "    MATCH (concept:Concept) WHERE concept.descriptionEmbedding IS NULL\n",
    "    WITH concept, genai.vector.encode(\n",
    "      concept.description, \n",
    "      \"OpenAI\", \n",
    "      {\n",
    "        token: $openAiApiKey\n",
    "      }) AS vector\n",
    "    CALL db.create.setNodeVectorProperty(concept, \"descriptionEmbedding\", vector)\n",
    "    \"\"\", \n",
    "    params={\"openAiApiKey\":OPENAI_API_KEY} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busqueda por Concept Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(question):\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  vector_search_query = \"\"\"\n",
    "    WITH genai.vector.encode(\n",
    "      $question, \n",
    "      \"OpenAI\", \n",
    "      {\n",
    "        token: $openAiApiKey\n",
    "      }) AS question_embedding\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, question_embedding) yield node, score\n",
    "    RETURN score, node.name AS text\n",
    "  \"\"\"\n",
    "  similar = kg.query(vector_search_query, \n",
    "                     params={\n",
    "                      'question': question, \n",
    "                      'openAiApiKey':OPENAI_API_KEY,\n",
    "                      'index_name':'vector_concepts', \n",
    "                      'top_k': 3})\n",
    "  return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = neo4j_vector_search(\n",
    "    'Cual es al elasticidad de mi promocion'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9372406005859375, 'text': 'Elasticidad'},\n",
       " {'score': 0.9013671875, 'text': 'accuracy'},\n",
       " {'score': 0.8984527587890625,\n",
       "  'text': 'Cuadrantes de desempeño de promociones'}]"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busqueda por Chunk Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search_chunks(question):\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  vector_search_query = \"\"\"\n",
    "    WITH genai.vector.encode(\n",
    "      $question, \n",
    "      \"OpenAI\", \n",
    "      {\n",
    "        token: $openAiApiKey\n",
    "      }) AS question_embedding\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, question_embedding) yield node, score\n",
    "    RETURN score, node.name AS text\n",
    "  \"\"\"\n",
    "  similar = kg.query(vector_search_query, \n",
    "                     params={\n",
    "                      'question': question, \n",
    "                      'openAiApiKey':OPENAI_API_KEY,\n",
    "                      'index_name':'vector_chunks', \n",
    "                      'top_k': 3})\n",
    "  return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_chunks = neo4j_vector_search_chunks(\n",
    "    'Que hace la empresa Arca Continental'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9392242431640625, 'text': 'company_info-chunk0000'},\n",
       " {'score': 0.9199676513671875, 'text': 'company_info-chunk0001'},\n",
       " {'score': 0.8665618896484375, 'text': 'elasticidad-chunk0002'}]"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps...\n",
    "\n",
    "- Find a way for deduplication\n",
    "- For new chunks if entities identified already exist try to concatenate only new data to the existing entity, avoid duplication\n",
    "- For other type of nodes is there is a exact coincidence in KG, concatenate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKFLOW FOR SMART DEDUPLICATION\n",
    "\n",
    "#Document is chunked - Completed\n",
    "#Entity nodes with property are extracted with LLM - Completed\n",
    "#Function that retrieve only Concept Nodes and their description - Completed\n",
    "#Cypher Query that compares each concept node name and vector description with graph - Completed\n",
    "#LLM that decides whether to concatenate new information with existing concept node or if it should be a new concept node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_nodes = []\n",
    "\n",
    "for node in nodes['nodes']:\n",
    "\n",
    "    node_type = node['type']\n",
    "    node_name = node['name']\n",
    "\n",
    "    if node_type == 'Concept':\n",
    "        concept_nodes.append(node)\n",
    "\n",
    "len(concept_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept node - Impacto de Canibalización\n",
      "[{'node.name': 'Impacto de Canibalización', 'node.description': 'El impacto de canibalización mide la pérdida causada por la disminución en ventas de otros productos debido a una promoción. Representa lo que se dejó de percibir de productos no promocionados cuando los clientes optaron por comprar el producto en promoción.', 'score': 0.996490478515625}, {'node.name': 'Canibalización', 'node.description': \"La canibalización en el contexto de una promoción se refiere al fenómeno en el que una acción promocional para un producto o servicio afecta negativamente las ventas de otros productos o servicios de la misma empresa. En otras palabras, es cuando una promoción 'se come' las ventas de otros productos.\", 'score': 0.9651641845703125}]\n",
      "concept node - Impacto de Overstock\n",
      "[{'node.name': 'Impacto de Overstock', 'node.description': 'El impacto de overstock mide la pérdida causada por el exceso de inventario generado después de una promoción. Representa lo que se dejó de vender debido a que los clientes compraron más producto del que normalmente consumirían durante la promoción (efecto de stockeo), reduciendo así sus compras en períodos posteriores.', 'score': 0.9959564208984375}]\n"
     ]
    }
   ],
   "source": [
    "for concept in concept_nodes:\n",
    "    node_name = concept['name']\n",
    "    node_description = concept['description']\n",
    "\n",
    "    similarity_threshold = 0.9\n",
    "    word_edit_distance = 5\n",
    "\n",
    "\n",
    "    data = kg.query(\"\"\"\n",
    "    WITH genai.vector.encode($description, \"OpenAI\", {token: $openAiApiKey}) AS description_embedding\n",
    "\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, description_embedding)\n",
    "    YIELD node, score\n",
    "\n",
    "    WHERE score > toFloat($cutoff)\n",
    "    AND (\n",
    "        toLower(node.name) CONTAINS toLower($name)\n",
    "        OR toLower($name) CONTAINS toLower(node.name)\n",
    "        OR apoc.text.distance(toLower(node.name), toLower($name)) < $distance\n",
    "    )\n",
    "\n",
    "    RETURN node.name, node.description, score\n",
    "    ORDER BY score DESC;\n",
    "\n",
    "    \"\"\", params={'name': node_name,\n",
    "                'description': node_description, \n",
    "                'openAiApiKey':OPENAI_API_KEY,\n",
    "                'index_name':'vector_concepts', \n",
    "                'top_k': 3,\n",
    "                'cutoff': similarity_threshold, \n",
    "                'distance': word_edit_distance})\n",
    "    \n",
    "\n",
    "    print(f'concept node - {node_name}')\n",
    "    print(data)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "{code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.queryNodes`: Caused by: java.lang.NullPointerException: 'query' must not be null}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[285], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m similarity_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m      4\u001b[0m word_edit_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mkg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43mMATCH (e:Concept)\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43mCALL \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m  WITH e\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m  CALL db.index.vector.queryNodes(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvector_concepts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, 10, e.descriptionEmbedding)\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m  YIELD node, score\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m  WITH node, score\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m  WHERE score > toFLoat($cutoff)\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m      AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m           OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m      AND labels(e) = labels(node)\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m  WITH node, score\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m  ORDER BY node.name\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m  RETURN collect(node) AS nodes\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m}\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43mWITH distinct nodes\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43mWHERE size(nodes) > 1\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43mWITH collect([n in nodes | n.name]) AS results\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43mUNWIND range(0, size(results)-1, 1) as index\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43mWITH results, index, results[index] as result\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43mWITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;43m        CASE WHEN index <> index2 AND\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43m            size(apoc.coll.intersection(acc, results[index2])) > 0\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43m            THEN apoc.coll.union(acc, results[index2])\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43m            ELSE acc\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43m        END\u001b[39;49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;43m)) as combinedResult\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;43mWITH distinct(combinedResult) as combinedResult\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;43m// extra filtering\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43mWITH collect(combinedResult) as allCombinedResults\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43mUNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43mWITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;43mWHERE NOT any(x IN range(0,size(allCombinedResults)-1,1) \u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;43m    WHERE x <> combinedResultIndex\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;43m    AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\u001b[39;49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;43mRETURN combinedResult  \u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcutoff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_edit_distance\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row)\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/langchain_community/graphs/neo4j_graph.py:431\u001b[0m, in \u001b[0;36mNeo4jGraph.query\u001b[0;34m(self, query, params)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneo4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Neo4jError\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     data, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msanitize:\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/driver.py:969\u001b[0m, in \u001b[0;36mDriver.execute_query\u001b[0;34m(self, query_, parameters_, routing_, database_, impersonated_user_, bookmark_manager_, auth_, result_transformer_, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    966\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid routing control value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrouting_\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    967\u001b[0m     )\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_pipelined_begin:\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_transaction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTelemetryAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDRIVER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_transformer_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/work/session.py:581\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m tx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtransaction_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# if cancellation callback has not been called yet:\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_work/query.py:144\u001b[0m, in \u001b[0;36munit_of_work.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/driver.py:1306\u001b[0m, in \u001b[0;36m_work\u001b[0;34m(tx, query, parameters, transformer)\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_work\u001b[39m(\n\u001b[1;32m   1300\u001b[0m     tx: ManagedTransaction,\n\u001b[1;32m   1301\u001b[0m     query: te\u001b[38;5;241m.\u001b[39mLiteralString,\n\u001b[1;32m   1302\u001b[0m     parameters: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny],\n\u001b[1;32m   1303\u001b[0m     transformer: t\u001b[38;5;241m.\u001b[39mCallable[[Result], t\u001b[38;5;241m.\u001b[39mUnion[_T]],\n\u001b[1;32m   1304\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m   1305\u001b[0m     res \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39mrun(query, parameters)\n\u001b[0;32m-> 1306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/work/result.py:797\u001b[0m, in \u001b[0;36mResult.to_eager_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;129m@NonConcurrentMethodChecker\u001b[39m\u001b[38;5;241m.\u001b[39m_non_concurrent_method\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_eager_result\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EagerResult:\n\u001b[1;32m    781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m    Convert this result to an :class:`.EagerResult`.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 5.8 Stabilized from experimental.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EagerResult(\n\u001b[1;32m    799\u001b[0m         keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m    800\u001b[0m         records\u001b[38;5;241m=\u001b[39mUtil\u001b[38;5;241m.\u001b[39mlist(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    801\u001b[0m         summary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsume(),\n\u001b[1;32m    802\u001b[0m     )\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/work/result.py:454\u001b[0m, in \u001b[0;36mResult._buffer_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_all\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/work/result.py:443\u001b[0m, in \u001b[0;36mResult._buffer\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m record_buffer \u001b[38;5;241m=\u001b[39m deque()\n\u001b[0;32m--> 443\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecord_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrecord_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/work/result.py:393\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:184\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:994\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    991\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    992\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    993\u001b[0m )\n\u001b[0;32m--> 994\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1204\u001b[0m, in \u001b[0;36mBolt5x7._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enrich_error_diagnostic_record(summary_metadata)\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1204\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[0;32m/opt/.expertai/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:254\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    252\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    253\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hydrate_error(metadata)\n",
      "\u001b[0;31mClientError\u001b[0m: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.queryNodes`: Caused by: java.lang.NullPointerException: 'query' must not be null}"
     ]
    }
   ],
   "source": [
    "#Check for common Concept Nodes using vector similarity and Levenstein distance\n",
    "\n",
    "similarity_threshold = 0.9\n",
    "word_edit_distance = 5\n",
    "\n",
    "data = kg.query(\"\"\"\n",
    "MATCH (e:Concept)\n",
    "CALL {\n",
    "  WITH e\n",
    "  CALL db.index.vector.queryNodes('vector_concepts', 10, e.descriptionEmbedding)\n",
    "  YIELD node, score\n",
    "  WITH node, score\n",
    "  WHERE score > toFLoat($cutoff)\n",
    "      AND (toLower(node.name) CONTAINS toLower(e.name) OR toLower(e.name) CONTAINS toLower(node.name)\n",
    "           OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\n",
    "      AND labels(e) = labels(node)\n",
    "  WITH node, score\n",
    "  ORDER BY node.name\n",
    "  RETURN collect(node) AS nodes\n",
    "}\n",
    "WITH distinct nodes\n",
    "WHERE size(nodes) > 1\n",
    "WITH collect([n in nodes | n.name]) AS results\n",
    "UNWIND range(0, size(results)-1, 1) as index\n",
    "WITH results, index, results[index] as result\n",
    "WITH apoc.coll.sort(reduce(acc = result, index2 IN range(0, size(results)-1, 1) |\n",
    "        CASE WHEN index <> index2 AND\n",
    "            size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "            THEN apoc.coll.union(acc, results[index2])\n",
    "            ELSE acc\n",
    "        END\n",
    ")) as combinedResult\n",
    "WITH distinct(combinedResult) as combinedResult\n",
    "// extra filtering\n",
    "WITH collect(combinedResult) as allCombinedResults\n",
    "UNWIND range(0, size(allCombinedResults)-1, 1) as combinedResultIndex\n",
    "WITH allCombinedResults[combinedResultIndex] as combinedResult, combinedResultIndex, allCombinedResults\n",
    "WHERE NOT any(x IN range(0,size(allCombinedResults)-1,1) \n",
    "    WHERE x <> combinedResultIndex\n",
    "    AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    ")\n",
    "RETURN combinedResult  \n",
    "\"\"\", params={'cutoff': similarity_threshold, 'distance': word_edit_distance})\n",
    "for row in data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (e) { ... }} {position: line: 4, column: 1, offset: 82} for query: \"\\nWITH $conceptName AS conceptNameParam\\nMATCH (e:Concept {name: conceptNameParam})\\nCALL {\\n  WITH e\\n  CALL db.index.vector.queryNodes('vector_concepts', 10, e.descriptionEmbedding)\\n  YIELD node, score\\n  WITH node, score, e\\n  WHERE score > toFloat($cutoff)\\n    AND (toLower(node.name) CONTAINS toLower(e.name) \\n         OR toLower(e.name) CONTAINS toLower(node.name)\\n         OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\\n    AND labels(e) = labels(node)\\n  WITH node, score\\n  ORDER BY node.name\\n  RETURN collect(node) AS nodes\\n}\\nWITH distinct nodes\\nWHERE size(nodes) > 1\\nWITH collect([n in nodes | n.name]) AS results\\nUNWIND range(0, size(results)-1) AS index\\nWITH results, index, results[index] AS result\\nWITH apoc.coll.sort(\\n  reduce(acc = result, index2 IN range(0, size(results)-1) |\\n    CASE WHEN index <> index2 AND size(apoc.coll.intersection(acc, results[index2])) > 0\\n         THEN apoc.coll.union(acc, results[index2])\\n         ELSE acc\\n    END\\n  )\\n) AS combinedResult\\nWITH distinct(combinedResult) AS combinedResult\\nWITH collect(combinedResult) AS allCombinedResults\\nUNWIND range(0, size(allCombinedResults)-1) AS combinedResultIndex\\nWITH allCombinedResults[combinedResultIndex] AS combinedResult, combinedResultIndex, allCombinedResults\\nWHERE NOT any(x IN range(0,size(allCombinedResults)-1) \\n    WHERE x <> combinedResultIndex\\n    AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\\n)\\nRETURN combinedResult\\n\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'combinedResult': ['Canibalización', 'Impacto de Canibalización']}\n"
     ]
    }
   ],
   "source": [
    "#Check for common Concept Nodes given as input the name of a potential node using vector similarity and Levenstein distance\n",
    "#Encodes the description of the node generated by LLM\n",
    "\n",
    "\n",
    "similarity_threshold = 0.9\n",
    "word_edit_distance = 5\n",
    "\n",
    "data = kg.query(\"\"\"\n",
    "WITH $conceptName AS conceptNameParam\n",
    "MATCH (e:Concept {name: conceptNameParam})\n",
    "CALL {\n",
    "  WITH e\n",
    "  CALL db.index.vector.queryNodes('vector_concepts', 10, e.descriptionEmbedding)\n",
    "  YIELD node, score\n",
    "  WITH node, score, e\n",
    "  WHERE score > toFloat($cutoff)\n",
    "    AND (toLower(node.name) CONTAINS toLower(e.name) \n",
    "         OR toLower(e.name) CONTAINS toLower(node.name)\n",
    "         OR apoc.text.distance(toLower(node.name), toLower(e.name)) < $distance)\n",
    "    AND labels(e) = labels(node)\n",
    "  WITH node, score\n",
    "  ORDER BY node.name\n",
    "  RETURN collect(node) AS nodes\n",
    "}\n",
    "WITH distinct nodes\n",
    "WHERE size(nodes) > 1\n",
    "WITH collect([n in nodes | n.name]) AS results\n",
    "UNWIND range(0, size(results)-1) AS index\n",
    "WITH results, index, results[index] AS result\n",
    "WITH apoc.coll.sort(\n",
    "  reduce(acc = result, index2 IN range(0, size(results)-1) |\n",
    "    CASE WHEN index <> index2 AND size(apoc.coll.intersection(acc, results[index2])) > 0\n",
    "         THEN apoc.coll.union(acc, results[index2])\n",
    "         ELSE acc\n",
    "    END\n",
    "  )\n",
    ") AS combinedResult\n",
    "WITH distinct(combinedResult) AS combinedResult\n",
    "WITH collect(combinedResult) AS allCombinedResults\n",
    "UNWIND range(0, size(allCombinedResults)-1) AS combinedResultIndex\n",
    "WITH allCombinedResults[combinedResultIndex] AS combinedResult, combinedResultIndex, allCombinedResults\n",
    "WHERE NOT any(x IN range(0,size(allCombinedResults)-1) \n",
    "    WHERE x <> combinedResultIndex\n",
    "    AND apoc.coll.containsAll(allCombinedResults[x], combinedResult)\n",
    ")\n",
    "RETURN combinedResult\n",
    "\n",
    "\"\"\", params={'cutoff': similarity_threshold, 'distance': word_edit_distance, 'conceptName': 'Canibalización'})\n",
    "for row in data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'node.name': 'Canibalización', 'node.description': \"La canibalización en el contexto de una promoción se refiere al fenómeno en el que una acción promocional para un producto o servicio afecta negativamente las ventas de otros productos o servicios de la misma empresa. En otras palabras, es cuando una promoción 'se come' las ventas de otros productos.\", 'score': 0.94403076171875}]\n"
     ]
    }
   ],
   "source": [
    "similarity_threshold = 0.9\n",
    "word_edit_distance = 5\n",
    "\n",
    "concept_node = concept_nodes[0]\n",
    "#node_name = concept_node['name']\n",
    "#node_description = concept_node['description']\n",
    "\n",
    "node_name = 'Canibalization'\n",
    "node_description = 'In the context of promotions, \"cannibalization\" refers to a situation where a newly introduced promotion—or a discounted offer on a specific product—leads customers to shift their purchases away from other items or full-priced products that the company already sells. Essentially, rather than increasing overall sales, the promotion diverts existing demand from one area of the product lineup to another.'\n",
    "\n",
    "data = kg.query(\"\"\"\n",
    "WITH genai.vector.encode($description, \"OpenAI\", {token: $openAiApiKey}) AS description_embedding\n",
    "\n",
    "CALL db.index.vector.queryNodes($index_name, $top_k, description_embedding)\n",
    "YIELD node, score\n",
    "\n",
    "WHERE score > toFloat($cutoff)\n",
    "  AND (\n",
    "    toLower(node.name) CONTAINS toLower($name)\n",
    "    OR toLower($name) CONTAINS toLower(node.name)\n",
    "    OR apoc.text.distance(toLower(node.name), toLower($name)) < $distance\n",
    "  )\n",
    "\n",
    "RETURN node.name, node.description, score\n",
    "ORDER BY score DESC;\n",
    "\n",
    "\"\"\", params={'name': node_name,\n",
    "             'description': node_description, \n",
    "              'openAiApiKey':OPENAI_API_KEY,\n",
    "              'index_name':'vector_concepts', \n",
    "              'top_k': 3,\n",
    "              'cutoff': similarity_threshold, \n",
    "              'distance': word_edit_distance})\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'node.name: Canibalization, node.description: In the context of promotions, \"cannibalization\" refers to a situation where a newly introduced promotion—or a discounted offer on a specific product—leads customers to shift their purchases away from other items or full-priced products that the company already sells. Essentially, rather than increasing overall sales, the promotion diverts existing demand from one area of the product lineup to another.'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_concept = (f'node.name: {node_name}, node.description: {node_description}')\n",
    "new_concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplication(\n",
    "    new_concept: str = \"\",\n",
    "    similar_concepts: list = [],\n",
    "    llm: \"RunnableChain\" = None,\n",
    ") -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    LLM compares a list of very similar concept nodes with a new concept node identified previously.\n",
    "\n",
    "    If LLM identifies new concept node as unique it will output the same name as it is.\n",
    "\n",
    "    If LLM identifies a existing concept node as the same as new concept node, \n",
    "    new concept node name will be added to the list of synonyms of the existing concept node and \n",
    "    the output will be the name of the existing concept node.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------.\n",
    "    new_concept : str\n",
    "        New Concept Entity identified \n",
    "    similar_concepts : list\n",
    "        list of similar existing concept nodes previously extracted using vector search and Lebenshtein\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Name of the concept node \n",
    "    \"\"\"\n",
    "\n",
    "    if llm is None:\n",
    "        raise ValueError(\"llm must be provided\")\n",
    "\n",
    "    persona = (\n",
    "        \"\"\"You are an experienced Entity Resolution recognizer.\n",
    "        You will be provided with a new concept extracted previously from a document,\n",
    "        and with a list of very similar concepts extracted from the knowledge graph.\n",
    "        Based on name and the given description of each one you will define if the\n",
    "        new concept provided is really a unique concept or if it should be added to\n",
    "        the synonym list of an existing concept node. \n",
    "        \n",
    "        After deciding You must output \n",
    "        ONLY the NAME of the concept.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    context = (\n",
    "        f\"\"\"\n",
    "        <List of Similar Existing Concept Nodes>\n",
    "        {similar_concepts}\n",
    "        </List of Similar Existing Concept Nodes>\n",
    "        \"\"\"\n",
    "        if similar_concepts\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    best_practices = \"\"\"\n",
    "    Here’s a comprehensive list of rules for deciding:\n",
    "\n",
    "    1. **  If you identify the new concept node as unique you will output the same name of the concept as it is.\n",
    "    2. ** If you identify a existing concept node as the same as new concept node, \n",
    "    new concept node name will be added to the list of synonyms of the existing concept node and \n",
    "    the output will be the name of the existing concept node.\n",
    "    3. ** Take careful consideration on the description of each concept, and if they talk about the same concept.\n",
    "    4. ** If the list of similar existing concept nodes is empty, then the output must be the name of the new concept.\n",
    "    5. ** ONLY OUTPUT THE NAME OF THE CONCEPT NODE DONT EXPLAIN THE REASONING\n",
    "\n",
    "    Example of a good output:\n",
    "\n",
    "    <new_concept>\n",
    "       node.name: Impacto de Canibalización, \n",
    "       node.description: El impacto de canibalización mide la pérdida causada por la disminución en ventas de otros \n",
    "       productos debido a una promoción. Representa lo que se dejó de percibir de productos no promocionados cuando los \n",
    "       clientes optaron por comprar el producto en promoción.\n",
    "    </new_concept>\n",
    "\n",
    "    <List of Similar Existing Concept Nodes>\n",
    "\n",
    "    [{'node.name': 'Impacto de Canibalización', \n",
    "      'node.description': 'El impacto de canibalización mide la pérdida causada por la disminución en ventas de otros \n",
    "      productos debido a una promoción. Representa lo que se dejó de percibir de productos no promocionados cuando los \n",
    "      clientes optaron por comprar el producto en promoción.',\n",
    "      'score': 0.996490478515625}, \n",
    "     {'node.name': 'Canibalización',\n",
    "      'node.description': \"La canibalización en el contexto de una promoción se refiere al fenómeno en el que una acción \n",
    "      promocional para un producto o servicio afecta negativamente las ventas de otros productos o servicios de la misma empresa. \n",
    "      En otras palabras, es cuando una promoción 'se come' las ventas de otros productos.\", \n",
    "      'score': 0.9651641845703125}]\n",
    "\n",
    "    </List of Similar Existing Concept Nodes>\n",
    "\n",
    "    EXPECTED OUTPUT:\n",
    "\n",
    "    Impacto de Canibalización\n",
    "\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "        <role>\n",
    "        {ROLE}\n",
    "        </role>\n",
    "        <best_practices>\n",
    "        {BEST_PRACTICES}\n",
    "        </best_practices>\n",
    "        <new_concept>\n",
    "        {NEW_CONCEPT}\n",
    "        </new_concept>\n",
    "        {context}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=user_prompt,\n",
    "        input_variables=[\"context\"],\n",
    "        partial_variables={\n",
    "            \"BEST_PRACTICES\": best_practices,\n",
    "            \"ROLE\": persona,\n",
    "            \"NEW_CONCEPT\": new_concept,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "        \n",
    "    response = chain.invoke({\"context\": context}).content.strip()\n",
    "\n",
    "    result = chain.invoke({\"context\": context})\n",
    "    print(result)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Canibalización' additional_kwargs={'usage': {'prompt_tokens': 973, 'completion_tokens': 9, 'total_tokens': 982}, 'stop_reason': 'end_turn', 'model_id': 'us.anthropic.claude-3-5-haiku-20241022-v1:0'} response_metadata={'usage': {'prompt_tokens': 973, 'completion_tokens': 9, 'total_tokens': 982}, 'stop_reason': 'end_turn', 'model_id': 'us.anthropic.claude-3-5-haiku-20241022-v1:0'} id='run-ab1c4516-5132-40ae-a6cf-914d7eec0d5c-0' usage_metadata={'input_tokens': 973, 'output_tokens': 9, 'total_tokens': 982}\n"
     ]
    }
   ],
   "source": [
    "response = deduplication(new_concept, data, llm=llm_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Canibalización'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".expertai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
