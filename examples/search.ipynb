{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/libraries/grafa/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grafa import get_db_engine \n",
    "from langchain_aws import ChatBedrock\n",
    "from grafa.settings import OPENAI_API_KEY\n",
    "\n",
    "\n",
    "from grafa.stage_search.utils import extract_chunk_data, concept_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/libraries/grafa/grafa/utils/graph_connection.py:17: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  kg = Neo4jGraph(\n"
     ]
    }
   ],
   "source": [
    "kg = get_db_engine()\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "llm_light = ChatBedrock(\n",
    "        model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "        model_kwargs=dict(temperature=0, max_tokens=4096),\n",
    "    )\n",
    "\n",
    "llm_light_nova = ChatBedrockConverse(\n",
    "        model=\"amazon.nova-micro-v1:0\"\n",
    "    )\n",
    "\n",
    "from grafa.stage_search.vector_search import two_stage_retrieval_parallel, advanced_search_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Cuales fueron mis mejores promociones?\", \"Dame la elasticidad promedio de productos para la region Occidente\"\n",
    "           , \"Cual fue el ROI de mi producto con mejor incremental porcentual\", \"Encuentra los 10 clientes con mejor desempeño promocional en Enero de 2023. Para todos esos, encuentra las 10 promciones que tuvieron mejor incremental, y dime como les fue en canibalizacion\",\n",
    "         \"Como puedo mejorar la canibalizacion de mis productos\", \"Como puedo medir mi desempeño promocional?\", \"Como mide ARCA sus productos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "llm = ChatBedrock(\n",
    "        model_id=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        model_kwargs=dict(temperature=0, max_tokens=4096),\n",
    "    )\n",
    "\n",
    "class OutputRetrievalEval(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model to validate and structure LLM evaluation output.\n",
    "    \"\"\"\n",
    "    better_response: int = Field(description=\"Better response can only be 1 or 2\")\n",
    "    confidence_score: int = Field(description=\"Confidence score between 1 and 100.\")\n",
    "    justification: str = Field(description=\"Explanation of why the selected result is better.\")\n",
    "\n",
    "    @field_validator(\"better_response\")\n",
    "    @classmethod\n",
    "    def validate_better_response(cls, value):\n",
    "        if value not in [1, 2]:\n",
    "            raise ValueError(\"better_response must be either 1 or 2\")\n",
    "        return value\n",
    "\n",
    "    @field_validator(\"confidence_score\")\n",
    "    @classmethod\n",
    "    def validate_confidence_score(cls, value):\n",
    "        if not (1 <= value <= 100):\n",
    "            raise ValueError(\"confidence_score must be between 1 and 100\")\n",
    "        return value\n",
    "\n",
    "\n",
    "def evaluate_retrieval_results(\n",
    "    query: str,\n",
    "    result_1: list, \n",
    "    result_2: list,\n",
    "    llm: \"RunnableChain\" = None,\n",
    ") -> OutputRetrievalEval:\n",
    "    \"\"\"\n",
    "    Evaluates two retrieval results using an LLM and determines which one provides a better response to the query.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The original user query.\n",
    "    - result_1 (list): The first retrieval result (e.g., from two_stage_retrieval_parallel).\n",
    "    - result_2 (list): The second retrieval result (e.g., from advanced_search_parallel).\n",
    "    - llm_api_key (str): OpenAI API key for calling GPT models.\n",
    "    - model_name (str): The LLM model to use (default: \"gpt-4\").\n",
    "\n",
    "    Returns:\n",
    "    - OutputRetrievalEval: A structured evaluation output containing:\n",
    "        - The better response (1 or 2)\n",
    "        - Confidence score (1-100)\n",
    "        - Justification for the choice\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the retrieval results for comparison\n",
    "    result_1_text = \"\\n\".join(result_1) if result_1 else \"No relevant information found.\"\n",
    "    result_2_text = \"\\n\".join(result_2) if result_2 else \"No relevant information found.\"\n",
    "\n",
    "    # Define LLM messages\n",
    "    PROMPT = \"\"\"You are an expert in information retrieval evaluation. Your task is to analyze two different search results \n",
    "    and determine which one provides a more complete, informative, and contextually relevant answer to the given query.\n",
    "\n",
    "    Consider the following factors:\n",
    "    1. **Relevance**: Does the response directly answer the query?\n",
    "    2. **Completeness**: Does it cover all key aspects of the topic?\n",
    "    3. **Context**: Does it provide additional helpful background information?\n",
    "    4. **Conciseness**: Does it avoid unnecessary or redundant information?\n",
    "\n",
    "    Here is the user query:\n",
    "    **Query:** {query}\n",
    "\n",
    "    Here are the two retrieved responses:\n",
    "\n",
    "    **Result 1:**\n",
    "    {result_1_text}\n",
    "\n",
    "    **Result 2:**\n",
    "    {result_2_text}\n",
    "\n",
    "    Please evaluate both responses and return a structured JSON object following the given format.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=OutputRetrievalEval)\n",
    "\n",
    "    values = {\n",
    "            \"query\": query,\n",
    "            \"result_1_text\": result_1,\n",
    "            \"result_2_text\": result_2,\n",
    "        }\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "    template=PROMPT + '\\n {format_instructions}',\n",
    "    input_variables=[\"query\", \"result_1_text\", \"result_2_text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    response = chain.invoke(values)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "query = \"What are the effects of climate change?\"\n",
    "result_1 = [\"Climate change causes rising sea levels.\", \"It increases global temperatures and causes extreme weather.\"]\n",
    "result_2 = [\"Global warming affects ecosystems.\", \"It leads to droughts, hurricanes, and sea ice loss.\"]\n",
    "\n",
    "evaluation_result = evaluate_retrieval_results(query, result_1, result_2, llm)\n",
    "\n",
    "# Print the structured output\n",
    "print(evaluation_result.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_methods(query, index_name, top_k, kg, llm, api_key):\n",
    "    results = {}\n",
    "\n",
    "    # Evaluate Two-Stage Retrieval\n",
    "    start_time = time.perf_counter()\n",
    "    two_stage_results = two_stage_retrieval_parallel(\n",
    "        question=query, index_name=index_name, top_k=top_k, kg=kg, api_key=api_key\n",
    "    )\n",
    "    two_stage_time = time.perf_counter() - start_time\n",
    "    results[\"two_stage\"] = {\"time\": two_stage_time, \"results\": two_stage_results, \"Chunks\": len(two_stage_results)}\n",
    "\n",
    "    # Evaluate Advanced Search\n",
    "    start_time = time.perf_counter()\n",
    "    advanced_results = advanced_search_parallel(\n",
    "        query=query, index_name=index_name, top_k=top_k, llm=llm, kg=kg, api_key=api_key, verbose=False\n",
    "    )\n",
    "    advanced_time = time.perf_counter() - start_time\n",
    "    results[\"advanced\"] = {\"time\": advanced_time, \"results\": advanced_results, \"Chunks\": len(advanced_results)}\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating query: Cuales fueron mis mejores promociones?\n",
      "Evaluating query: Dame la elasticidad promedio de productos para la region Occidente\n",
      "Evaluating query: Cual fue el ROI de mi producto con mejor incremental porcentual\n",
      "Evaluating query: Encuentra los 10 clientes con mejor desempeño promocional en Enero de 2023. Para todos esos, encuentra las 10 promciones que tuvieron mejor incremental, y dime como les fue en canibalizacion\n",
      "Evaluating query: Como puedo mejorar la canibalizacion de mis productos\n",
      "Evaluating query: Como puedo medir mi desempeño promocional?\n",
      "Evaluating query: Como mide ARCA sus productos\n"
     ]
    }
   ],
   "source": [
    "llm = llm_light_nova\n",
    "api_key = OPENAI_API_KEY\n",
    "index_name = \"vector_concepts\"\n",
    "top_k = 10\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Evaluating query: {query}\")\n",
    "    results_dict[query] = evaluate_methods(query, index_name, top_k, kg, llm, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Cuales fueron mis mejores promociones?\n",
      "Overlap: 25.00%\n",
      "Query: Dame la elasticidad promedio de productos para la region Occidente\n",
      "Overlap: 38.46%\n",
      "Query: Cual fue el ROI de mi producto con mejor incremental porcentual\n",
      "Overlap: 19.05%\n",
      "Query: Encuentra los 10 clientes con mejor desempeño promocional en Enero de 2023. Para todos esos, encuentra las 10 promciones que tuvieron mejor incremental, y dime como les fue en canibalizacion\n",
      "Overlap: 31.58%\n",
      "Query: Como puedo mejorar la canibalizacion de mis productos\n",
      "Overlap: 20.00%\n",
      "Query: Como puedo medir mi desempeño promocional?\n",
      "Overlap: 33.33%\n",
      "Query: Como mide ARCA sus productos\n",
      "Overlap: 36.36%\n"
     ]
    }
   ],
   "source": [
    "def compute_overlap(results1, results2):\n",
    "    \"\"\"Calculate the percentage of overlap between two result sets.\"\"\"\n",
    "    set1, set2 = set(results1), set(results2)\n",
    "    if not set1 and not set2:\n",
    "        return 100  # Both are empty\n",
    "    if not set1 or not set2:\n",
    "        return 0  # One is empty\n",
    "    return len(set1 & set2) / len(set1 | set2) * 100  # Jaccard Similarity %\n",
    "\n",
    "for query in queries:\n",
    "    two_stage_res = results_dict[query][\"two_stage\"][\"results\"]\n",
    "    advanced_res = results_dict[query][\"advanced\"][\"results\"]\n",
    "\n",
    "    overlap = compute_overlap(two_stage_res, advanced_res)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Overlap: {overlap:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Two-Stage Time (s)</th>\n",
       "      <th>Advanced Time (s)</th>\n",
       "      <th>Overlap (%)</th>\n",
       "      <th>Two-Stage Chunks</th>\n",
       "      <th>Advanced Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuales fueron mis mejores promociones?</td>\n",
       "      <td>2.888705</td>\n",
       "      <td>1.687309</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dame la elasticidad promedio de productos para...</td>\n",
       "      <td>1.073278</td>\n",
       "      <td>1.639234</td>\n",
       "      <td>38.461538</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cual fue el ROI de mi producto con mejor incre...</td>\n",
       "      <td>1.455424</td>\n",
       "      <td>1.668549</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Encuentra los 10 clientes con mejor desempeño ...</td>\n",
       "      <td>1.956123</td>\n",
       "      <td>1.964452</td>\n",
       "      <td>31.578947</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Como puedo mejorar la canibalizacion de mis pr...</td>\n",
       "      <td>1.136357</td>\n",
       "      <td>1.267598</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Como puedo medir mi desempeño promocional?</td>\n",
       "      <td>1.601835</td>\n",
       "      <td>1.538107</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Como mide ARCA sus productos</td>\n",
       "      <td>1.075828</td>\n",
       "      <td>1.312858</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  Two-Stage Time (s)  \\\n",
       "0             Cuales fueron mis mejores promociones?            2.888705   \n",
       "1  Dame la elasticidad promedio de productos para...            1.073278   \n",
       "2  Cual fue el ROI de mi producto con mejor incre...            1.455424   \n",
       "3  Encuentra los 10 clientes con mejor desempeño ...            1.956123   \n",
       "4  Como puedo mejorar la canibalizacion de mis pr...            1.136357   \n",
       "5         Como puedo medir mi desempeño promocional?            1.601835   \n",
       "6                       Como mide ARCA sus productos            1.075828   \n",
       "\n",
       "   Advanced Time (s)  Overlap (%)  Two-Stage Chunks  Advanced Chunks  \n",
       "0           1.687309    25.000000                 8                2  \n",
       "1           1.639234    38.461538                 9                9  \n",
       "2           1.668549    19.047619                11               14  \n",
       "3           1.964452    31.578947                10               15  \n",
       "4           1.267598    20.000000                11                7  \n",
       "5           1.538107    33.333333                10                6  \n",
       "6           1.312858    36.363636                10                5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for query in queries:\n",
    "    two_stage_time = results_dict[query][\"two_stage\"][\"time\"]\n",
    "    advanced_time = results_dict[query][\"advanced\"][\"time\"]\n",
    "    overlap = compute_overlap(\n",
    "        results_dict[query][\"two_stage\"][\"results\"],\n",
    "        results_dict[query][\"advanced\"][\"results\"]\n",
    "    )\n",
    "    two_stage_chunks = results_dict[query][\"two_stage\"][\"Chunks\"]\n",
    "    advanced_chunks = results_dict[query][\"advanced\"][\"Chunks\"]\n",
    "\n",
    "    comparison_data.append({\n",
    "        \"Query\": query,\n",
    "        \"Two-Stage Time (s)\": two_stage_time,\n",
    "        \"Advanced Time (s)\": advanced_time,\n",
    "        \"Overlap (%)\": overlap,\n",
    "        \"Two-Stage Chunks\": two_stage_chunks,\n",
    "        \"Advanced Chunks\": advanced_chunks\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Cuales fueron mis mejores promociones?\", \"Dame la elasticidad promedio de productos para la region Occidente\"\n",
    "           , \"Cual fue el ROI de mi producto con mejor incremental porcentual\", \"Encuentra los 10 clientes con mejor desempeño promocional en Enero de 2023. Para todos esos, encuentra las 10 promciones que tuvieron mejor incremental, y dime como les fue en canibalizacion\",\n",
    "         \"Como puedo mejorar la canibalizacion de mis productos\", \"Como puedo medir mi desempeño promocional?\", \"Como mide ARCA sus productos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    model_kwargs=dict(temperature=0, max_tokens=4096),\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Pydantic Model for LLM Evaluation ----\n",
    "class OutputRetrievalEval(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model to validate and structure LLM evaluation output.\n",
    "    \"\"\"\n",
    "    better_response: int = Field(description=\"Better response can only be 1 or 2\")\n",
    "    confidence_score: int = Field(description=\"Confidence score between 1 and 100.\")\n",
    "    justification: str = Field(description=\"Explanation of why the selected result is better.\")\n",
    "\n",
    "    @field_validator(\"better_response\")\n",
    "    @classmethod\n",
    "    def validate_better_response(cls, value):\n",
    "        if value not in [1, 2]:\n",
    "            raise ValueError(\"better_response must be either 1 or 2\")\n",
    "        return value\n",
    "\n",
    "    @field_validator(\"confidence_score\")\n",
    "    @classmethod\n",
    "    def validate_confidence_score(cls, value):\n",
    "        if not (1 <= value <= 100):\n",
    "            raise ValueError(\"confidence_score must be between 1 and 100\")\n",
    "        return value\n",
    "\n",
    "\n",
    "# ---- Function to Evaluate Retrieval Results ----\n",
    "def evaluate_retrieval_results(\n",
    "    query: str,\n",
    "    result_1: list, \n",
    "    result_2: list,\n",
    "    llm: \"RunnableChain\",\n",
    ") -> OutputRetrievalEval:\n",
    "    \"\"\"\n",
    "    Evaluates two retrieval results using an LLM and determines which one provides a better response to the query.\n",
    "    \"\"\"\n",
    "\n",
    "    result_1_text = \"\\n\".join(result_1) if result_1 else \"No relevant information found.\"\n",
    "    result_2_text = \"\\n\".join(result_2) if result_2 else \"No relevant information found.\"\n",
    "\n",
    "    # Define LLM messages\n",
    "    PROMPT = \"\"\"You are an expert in information retrieval evaluation. Your task is to analyze two different search results \n",
    "    and determine which one provides a more complete, informative, and contextually relevant answer to the given query. The best result \n",
    "    should give contextual informaiton for example what is certain metric, how to calculate it, formula, etc. Or what a concept means, it must\n",
    "    give more CONTEXT for a further SQL query.\n",
    "\n",
    "\n",
    "    Consider the following factors:\n",
    "    1. **Relevance**: Does the response directly answer the query?\n",
    "    2. **Completeness**: Does it cover all key aspects of the topic?\n",
    "    3. **Context**: Does it provide additional helpful background information?\n",
    "    4. **Conciseness**: Does it avoid unnecessary or redundant information?\n",
    "\n",
    "    Here is the user query:\n",
    "    **Query:** {query}\n",
    "\n",
    "    Here are the two retrieved responses:\n",
    "\n",
    "    **Result 1:**\n",
    "    {result_1_text}\n",
    "\n",
    "    **Result 2:**\n",
    "    {result_2_text}\n",
    "\n",
    "    Please evaluate both responses and return a structured JSON object following the given format.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=OutputRetrievalEval)\n",
    "\n",
    "    values = {\n",
    "        \"query\": query,\n",
    "        \"result_1_text\": result_1,\n",
    "        \"result_2_text\": result_2,\n",
    "    }\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=PROMPT + '\\n {format_instructions}',\n",
    "        input_variables=[\"query\", \"result_1_text\", \"result_2_text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    response = chain.invoke(values)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# ---- Function to Retrieve and Evaluate Search Results ----\n",
    "def evaluate_methods(query, index_name, top_k, kg, llm, api_key):\n",
    "    results = {}\n",
    "\n",
    "    # Evaluate Two-Stage Retrieval\n",
    "    start_time = time.perf_counter()\n",
    "    two_stage_results = two_stage_retrieval_parallel(\n",
    "        question=query, index_name=index_name, top_k=top_k, kg=kg, api_key=api_key\n",
    "    )\n",
    "    two_stage_time = time.perf_counter() - start_time\n",
    "    results[\"two_stage\"] = {\"time\": two_stage_time, \"results\": two_stage_results, \"Chunks\": len(two_stage_results)}\n",
    "\n",
    "    # Evaluate Advanced Search\n",
    "    start_time = time.perf_counter()\n",
    "    advanced_results = advanced_search_parallel(\n",
    "        query=query, index_name=index_name, top_k=top_k, llm=llm_light_nova, kg=kg, api_key=api_key, verbose=False\n",
    "    )\n",
    "    advanced_time = time.perf_counter() - start_time\n",
    "    results[\"advanced\"] = {\"time\": advanced_time, \"results\": advanced_results, \"Chunks\": len(advanced_results)}\n",
    "\n",
    "    # Evaluate with LLM\n",
    "    evaluation_result = evaluate_retrieval_results(query, two_stage_results, advanced_results, llm)\n",
    "\n",
    "    results[\"evaluation\"] = evaluation_result\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating query: Cuales fueron mis mejores promociones?\n",
      "Evaluating query: Dame la elasticidad promedio de productos para la region Occidente\n",
      "Evaluating query: Cual fue el ROI de mi producto con mejor incremental porcentual\n",
      "Evaluating query: Encuentra los 10 clientes con mejor desempeño promocional en Enero de 2023. Para todos esos, encuentra las 10 promciones que tuvieron mejor incremental, y dime como les fue en canibalizacion\n",
      "Evaluating query: Como puedo mejorar la canibalizacion de mis productos\n",
      "Evaluating query: Como puedo medir mi desempeño promocional?\n",
      "Evaluating query: Como mide ARCA sus productos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Two-Stage Time (s)</th>\n",
       "      <th>Advanced Time (s)</th>\n",
       "      <th>Overlap (%)</th>\n",
       "      <th>Two-Stage Chunks</th>\n",
       "      <th>Advanced Chunks</th>\n",
       "      <th>Better Response</th>\n",
       "      <th>Confidence Score</th>\n",
       "      <th>Justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuales fueron mis mejores promociones?</td>\n",
       "      <td>1.562294</td>\n",
       "      <td>1.492275</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>Result 1 provides a more comprehensive and det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dame la elasticidad promedio de productos para...</td>\n",
       "      <td>1.487575</td>\n",
       "      <td>1.794277</td>\n",
       "      <td>38.461538</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>85</td>\n",
       "      <td>Result 2 provides a more complete and contextu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cual fue el ROI de mi producto con mejor incre...</td>\n",
       "      <td>1.306277</td>\n",
       "      <td>1.755498</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>Result 1 is better for several reasons: 1) It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Encuentra los 10 clientes con mejor desempeño ...</td>\n",
       "      <td>1.528816</td>\n",
       "      <td>2.080351</td>\n",
       "      <td>31.578947</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>Result 1 provides a more comprehensive and rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Como puedo mejorar la canibalizacion de mis pr...</td>\n",
       "      <td>1.155370</td>\n",
       "      <td>1.514676</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>Result 1 provides a more comprehensive and wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Como puedo medir mi desempeño promocional?</td>\n",
       "      <td>1.227443</td>\n",
       "      <td>1.600392</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>Result 1 provides a more comprehensive and str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Como mide ARCA sus productos</td>\n",
       "      <td>1.694415</td>\n",
       "      <td>1.438592</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>Result 1 provides a more complete and relevant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  Two-Stage Time (s)  \\\n",
       "0             Cuales fueron mis mejores promociones?            1.562294   \n",
       "1  Dame la elasticidad promedio de productos para...            1.487575   \n",
       "2  Cual fue el ROI de mi producto con mejor incre...            1.306277   \n",
       "3  Encuentra los 10 clientes con mejor desempeño ...            1.528816   \n",
       "4  Como puedo mejorar la canibalizacion de mis pr...            1.155370   \n",
       "5         Como puedo medir mi desempeño promocional?            1.227443   \n",
       "6                       Como mide ARCA sus productos            1.694415   \n",
       "\n",
       "   Advanced Time (s)  Overlap (%)  Two-Stage Chunks  Advanced Chunks  \\\n",
       "0           1.492275    25.000000                 8                2   \n",
       "1           1.794277    38.461538                 9                9   \n",
       "2           1.755498    19.047619                11               14   \n",
       "3           2.080351    31.578947                10               15   \n",
       "4           1.514676    20.000000                11                7   \n",
       "5           1.600392    33.333333                10                6   \n",
       "6           1.438592    36.363636                10                5   \n",
       "\n",
       "   Better Response  Confidence Score  \\\n",
       "0                1                85   \n",
       "1                2                85   \n",
       "2                1                85   \n",
       "3                1                85   \n",
       "4                1                85   \n",
       "5                1                85   \n",
       "6                1                85   \n",
       "\n",
       "                                       Justification  \n",
       "0  Result 1 provides a more comprehensive and det...  \n",
       "1  Result 2 provides a more complete and contextu...  \n",
       "2  Result 1 is better for several reasons: 1) It ...  \n",
       "3  Result 1 provides a more comprehensive and rel...  \n",
       "4  Result 1 provides a more comprehensive and wel...  \n",
       "5  Result 1 provides a more comprehensive and str...  \n",
       "6  Result 1 provides a more complete and relevant...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- Running Multiple Queries and Storing Evaluations ----\n",
    "api_key = OPENAI_API_KEY\n",
    "index_name = \"vector_concepts\"\n",
    "top_k = 10\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"Evaluating query: {query}\")\n",
    "    results_dict[query] = evaluate_methods(query, index_name, top_k, kg, llm, api_key)\n",
    "\n",
    "\n",
    "# ---- Compare and Display Results ----\n",
    "comparison_data = []\n",
    "\n",
    "for query in queries:\n",
    "    two_stage_time = results_dict[query][\"two_stage\"][\"time\"]\n",
    "    advanced_time = results_dict[query][\"advanced\"][\"time\"]\n",
    "    overlap = compute_overlap(\n",
    "        results_dict[query][\"two_stage\"][\"results\"],\n",
    "        results_dict[query][\"advanced\"][\"results\"]\n",
    "    )\n",
    "    two_stage_chunks = results_dict[query][\"two_stage\"][\"Chunks\"]\n",
    "    advanced_chunks = results_dict[query][\"advanced\"][\"Chunks\"]\n",
    "    eval_result = results_dict[query][\"evaluation\"]\n",
    "\n",
    "    comparison_data.append({\n",
    "        \"Query\": query,\n",
    "        \"Two-Stage Time (s)\": two_stage_time,\n",
    "        \"Advanced Time (s)\": advanced_time,\n",
    "        \"Overlap (%)\": overlap,\n",
    "        \"Two-Stage Chunks\": two_stage_chunks,\n",
    "        \"Advanced Chunks\": advanced_chunks,\n",
    "        \"Better Response\": eval_result.better_response,\n",
    "        \"Confidence Score\": eval_result.confidence_score,\n",
    "        \"Justification\": eval_result.justification\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Cuales fueron mis mejores promociones? -> Result: ['promociones'] (Time taken: 0.539630 sec)\n",
      "Query: Dame la elasticidad promedio de productos para la region Occidente -> Result: ['elasticidad', 'productos', 'region', 'Occidente'] (Time taken: 0.346204 sec)\n",
      "Query: Cual fue el ROI de mi producto con mejor incremental porcentual -> Result: ['ROI', 'producto', 'incremental', 'porcentual'] (Time taken: 0.336035 sec)\n",
      "Query: Encuentra los 10 clientes con mejor desempeño promocional en Enero de 2023. Para todos esos, encuentra las 10 promciones que tuvieron mejor incremental, y dime como les fue en canibalizacion -> Result: ['clientes', 'desempeño', 'promocional', 'promciones', 'incremental', 'canibalizacion'] (Time taken: 0.442461 sec)\n",
      "Query: Como puedo mejorar la canibalizacion de mis productos -> Result: ['canibalizacion', 'productos'] (Time taken: 0.301625 sec)\n",
      "Query: Como puedo medir mi desempeño promocional? -> Result: ['desempeño', 'promocional'] (Time taken: 0.282784 sec)\n",
      "\n",
      "Total execution time: 2.250268 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start overall timer\n",
    "overall_start_time = time.perf_counter()\n",
    "\n",
    "for query in queries:\n",
    "    start_time = time.perf_counter()  # Start time for this query\n",
    "    result = concept_extractor(query, llm_light_nova)  # Run function\n",
    "    end_time = time.perf_counter()  # End time for this query\n",
    "\n",
    "    elapsed_time = end_time - start_time  # Time for this query\n",
    "    print(f\"Query: {query} -> Result: {result} (Time taken: {elapsed_time:.6f} sec)\")\n",
    "\n",
    "# End overall timer\n",
    "overall_end_time = time.perf_counter()\n",
    "total_time = overall_end_time - overall_start_time\n",
    "\n",
    "print(f\"\\nTotal execution time: {total_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique concept names extracted: {'Elasticidad cruzada', 'Cuadrantes de desempeño de promociones', 'ROI', 'Optimización de precios', 'Incremental', 'Price Index', 'Impacto de Overstock', 'Precisión', 'Elasticidad', 'Canibalización'}\n",
      "Extracting data for chunk: accuracy-chunk0001\n",
      "Extracting data for chunk: cuadrantes-chunk0000\n",
      "Extracting data for chunk: canibalizacion-chunk0001\n",
      "Extracting data for chunk: accuracy-chunk0000\n",
      "Extracting data for chunk: kuona_test-chunk0000\n",
      "Extracting data for chunk: incremental-chunk0000\n",
      "Extracting data for chunk: canibalizacion-chunk0002\n",
      "Extracting data for chunk: cuadrantes-chunk0001\n",
      "Extracting data for chunk: elasticidad-chunk0002\n",
      "Extracting data for chunk: impact-chunk0001\n",
      "Extracting data for chunk: roi-chunk0000\n",
      "Extracting data for chunk: kuona_test-chunk0002\n",
      "Extracting data for chunk: roi-chunk0001\n",
      "Extracting data for chunk: kuona_test-chunk0001\n"
     ]
    }
   ],
   "source": [
    "from grafa.stage_search.vector_search import two_stage_retrieval_parallel\n",
    "\n",
    "query= \"Encuentra los 10 clientes con mejor desempeño promocional en Enero de 2023. Para todos esos, encuentra las 10 promciones que tuvieron mejor incremental, y dime como les fue en canibalizacion\"\n",
    "results = two_stage_retrieval_parallel(query, \"vector_concepts\", 10, kg, OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique concept names extracted: {'Cuadrantes de desempeño de promociones', 'Unidades Operativas', 'Portafolio de Productos', 'Elasticidad Elástica', 'Elasticidad Unitaria', 'Incremental', 'Impacto de Overstock', 'Elasticidad precio de la demanda', 'Elasticidad', 'Elasticidad Inelástica'}\n",
      "Extracting data for chunk: cuadrantes-chunk0001\n",
      "Extracting data for chunk: elasticidad-chunk0000\n",
      "Extracting data for chunk: company_info-chunk0000\n",
      "Extracting data for chunk: elasticidad-chunk0002\n",
      "Extracting data for chunk: elasticidad-chunk0001\n",
      "Extracting data for chunk: cuadrantes-chunk0000\n",
      "Extracting data for chunk: impact-chunk0001\n",
      "Extracting data for chunk: incremental-chunk0000\n"
     ]
    }
   ],
   "source": [
    "from grafa.stage_search.vector_search import two_stage_retrieval_parallel\n",
    "\n",
    "query=  \"Dame la elasticidad promedio de productos para la region Occidente\"\n",
    "results = two_stage_retrieval_parallel(query, \"vector_concepts\", 10, kg, OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting advanced search...\n",
      "Extracting relevant concepts...\n",
      "Found 3 unique relevant concepts: {'desempeño', 'medir', 'promocional'}\n",
      "Unique graph concept names extracted: {'Cuadrantes de desempeño de promociones', 'Promocode', 'Catálogo Promocional', 'Propósitos Promocionales'}\n",
      "Unique chunk names found: {'customer_success_master_file-chunk0023', 'cuadrantes-chunk0001', 'customer_success_master_file-chunk0022', 'customer_success_master_file-chunk0010', 'customer_success_master_file-chunk0045', 'cuadrantes-chunk0000'}\n",
      "Extracting data for chunk: cuadrantes-chunk0000\n",
      "Extracting data for chunk: cuadrantes-chunk0001\n",
      "Extracting data for chunk: customer_success_master_file-chunk0045\n",
      "Extracting data for chunk: customer_success_master_file-chunk0023\n",
      "Extracting data for chunk: customer_success_master_file-chunk0010\n",
      "Extracting data for chunk: customer_success_master_file-chunk0022\n",
      "Advanced search completed.\n"
     ]
    }
   ],
   "source": [
    "from grafa.stage_search.vector_search import advanced_search_parallel\n",
    "\n",
    "query=  \"Como puedo medir mi desempeño promocional?\"\n",
    "results = advanced_search_parallel(query, \"vector_concepts\", 10, llm_light_nova, kg, verbose=True, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='739f14be-c1e0-4b04-ae04-ffaa9931a41e' results=[V2RerankResponseResultsItem(document=None, index=0, relevance_score=0.8332299), V2RerankResponseResultsItem(document=None, index=3, relevance_score=0.78749067), V2RerankResponseResultsItem(document=None, index=2, relevance_score=0.10023405)] meta=ApiMeta(api_version=ApiMetaApiVersion(version='2', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=None, output_tokens=None, search_units=1.0, classifications=None), tokens=None, warnings=None)\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2(api_key=\"GW1HwTTDzyX50jIz3uCPAQdmcr6uLIT3CbP7ljzf\")\n",
    "\n",
    "\n",
    "response = co.rerank(\n",
    "    model=\"rerank-v3.5\",\n",
    "    query=query,\n",
    "    documents=results,\n",
    "    top_n=3,\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
